{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeveloperPratim/sses_final/blob/main/Part%20222%20Evolution%20Data%20Process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "!pip install git+https://github.com/huggingface/transformers.git@72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
        "\n",
        "!pip install git+https://github.com/casper-hansen/AutoAWQ.git@1c5ccc791fa2cb0697db3b4070df1813f1736208\n",
        "!pip install pandas openpyxl\n",
        "\n",
        "\n",
        "!pip install googlesearch-python\n",
        "\n",
        "\n",
        "!pip install autoawq"
      ],
      "metadata": {
        "id": "dhKu56rhFv-M",
        "outputId": "4d89b3d6-9fc0-46f9-98b1-081eb739f0c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git@72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
            "  Cloning https://github.com/huggingface/transformers.git (to revision 72958fcd3c98a7afdc61f953aa58c544ebda2f79) to /tmp/pip-req-build-avo7ur_a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-avo7ur_a\n",
            "  Running command git rev-parse -q --verify 'sha^72958fcd3c98a7afdc61f953aa58c544ebda2f79'\n",
            "  Running command git fetch -q https://github.com/huggingface/transformers.git 72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
            "  Running command git checkout -q 72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (2.32.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.0.dev0)\n",
            "  Downloading tokenizers-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (4.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.0.dev0)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0.dev0) (2024.12.14)\n",
            "Downloading tokenizers-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.34.0.dev0-py3-none-any.whl size=7746278 sha256=4d15b570f63eecc72bf3040699c9bb952974d52dd83a704a48ce4e68e2644b8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/7e/b3/04336a04924aa0de029f81a87788444ff8202f8eeeb389c534\n",
            "Successfully built transformers\n",
            "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.27.1\n",
            "    Uninstalling huggingface-hub-0.27.1:\n",
            "      Successfully uninstalled huggingface-hub-0.27.1\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.47.1\n",
            "    Uninstalling transformers-4.47.1:\n",
            "      Successfully uninstalled transformers-4.47.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "sentence-transformers 3.3.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.34.0.dev0 which is incompatible.\n",
            "accelerate 1.2.1 requires huggingface-hub>=0.21.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "diffusers 0.32.2 requires huggingface-hub>=0.23.2, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.17.3 tokenizers-0.14.1 transformers-4.34.0.dev0\n",
            "Collecting git+https://github.com/casper-hansen/AutoAWQ.git@1c5ccc791fa2cb0697db3b4070df1813f1736208\n",
            "  Cloning https://github.com/casper-hansen/AutoAWQ.git (to revision 1c5ccc791fa2cb0697db3b4070df1813f1736208) to /tmp/pip-req-build-02ucy9x4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/casper-hansen/AutoAWQ.git /tmp/pip-req-build-02ucy9x4\n",
            "  Running command git rev-parse -q --verify 'sha^1c5ccc791fa2cb0697db3b4070df1813f1736208'\n",
            "  Running command git fetch -q https://github.com/casper-hansen/AutoAWQ.git 1c5ccc791fa2cb0697db3b4070df1813f1736208\n",
            "  Running command git checkout -q 1c5ccc791fa2cb0697db3b4070df1813f1736208\n",
            "  Resolved https://github.com/casper-hansen/AutoAWQ.git to commit 1c5ccc791fa2cb0697db3b4070df1813f1736208\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (4.12.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (2024.12.14)\n",
            "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Installing collected packages: googlesearch-python\n",
            "Successfully installed googlesearch-python-1.3.0\n",
            "Collecting autoawq\n",
            "  Downloading autoawq-0.2.8.tar.gz (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from autoawq) (2.5.1+cu121)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (from autoawq) (3.1.0)\n",
            "Collecting transformers<=4.47.1,>=4.45.0 (from autoawq)\n",
            "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from autoawq) (0.14.1)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from autoawq) (4.12.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from autoawq) (1.2.1)\n",
            "Collecting datasets>=2.20 (from autoawq)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from autoawq) (0.23.0)\n",
            "Collecting huggingface_hub>=0.26.5 (from autoawq)\n",
            "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.20->autoawq)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (4.67.1)\n",
            "Collecting xxhash (from datasets>=2.20->autoawq)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.20->autoawq)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.20->autoawq)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (3.11.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (6.0.2)\n",
            "INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tokenizers>=0.12.1 (from autoawq)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (12.1.105)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->autoawq) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.5.1->autoawq) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.5.1->autoawq) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.47.1,>=4.45.0->autoawq) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<=4.47.1,>=4.45.0->autoawq) (0.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->autoawq) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.5.1->autoawq) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20->autoawq) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: autoawq\n",
            "  Building wheel for autoawq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autoawq: filename=autoawq-0.2.8-py3-none-any.whl size=108744 sha256=e8581cb9788f9b697f7089ac98d72040c80e4204989352df8d7fdeac06b82eaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/03/fe/99c1c678bfe8aca712186466969ed866f52feda95ae1dcd1b1\n",
            "Successfully built autoawq\n",
            "Installing collected packages: xxhash, fsspec, dill, multiprocess, huggingface_hub, tokenizers, transformers, datasets, autoawq\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.17.3\n",
            "    Uninstalling huggingface-hub-0.17.3:\n",
            "      Successfully uninstalled huggingface-hub-0.17.3\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.14.1\n",
            "    Uninstalling tokenizers-0.14.1:\n",
            "      Successfully uninstalled tokenizers-0.14.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.34.0.dev0\n",
            "    Uninstalling transformers-4.34.0.dev0:\n",
            "      Successfully uninstalled transformers-4.34.0.dev0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed autoawq-0.2.8 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 huggingface_hub-0.27.1 multiprocess-0.70.16 tokenizers-0.21.0 transformers-4.47.1 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9dRne2i4NIRZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "outputId": "a5efd89c-958f-4c58-8523-81dd2af78f7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access denied for https://www.reddit.com/r/learnprogramming/comments/o1snxx/operating_systems_act_as_an_interface_between_the/\n",
            "Access denied for https://quizlet.com/in/862388068/operating-system-flash-cards/\n",
            "Access denied for https://quizlet.com/100199142/computer-chapter-3-4-flash-cards/\n",
            "Access denied for https://brainly.com/question/41659019\n",
            "Access denied for https://brainly.in/question/56057439\n",
            "Access denied for https://www.pcrefix.co.uk/can-computers-run-without-operating-systems\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TimeoutError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/client_reqrep.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, connection)\u001b[0m\n\u001b[1;32m   1058\u001b[0m                     \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protocol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m                     \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttpProcessingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/streams.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCancelledError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: ",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-238bc223d5a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \"\"\"\n\u001b[1;32m    163\u001b[0m \u001b[0;31m# Running the plagiarism checker asynchronously\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0mresults_plagiarism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mplagiarism_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m# Printing the results in a readable format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-238bc223d5a6>\u001b[0m in \u001b[0;36mplagiarism_checker\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;34m\"\"\"Run plagiarism check on each sentence in the input text.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck_sentence_plagiarism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-238bc223d5a6>\u001b[0m in \u001b[0;36mcheck_sentence_plagiarism\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0maiohttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClientSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfetch_web_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-238bc223d5a6>\u001b[0m in \u001b[0;36mfetch_web_content\u001b[0;34m(url, session)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_useragent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Access denied for {url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/client.py\u001b[0m in \u001b[0;36m__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__aenter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_RetType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_RetType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__aenter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size)\u001b[0m\n\u001b[1;32m    728\u001b[0m                             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                                 \u001b[0;32mawait\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m                             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                                 \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/client_reqrep.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, connection)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m                 \u001b[0;31m# read response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/helpers.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0menter_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muncancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cancelling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import hashlib\n",
        "import json\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from typing import List, Dict, Any\n",
        "from googlesearch import search\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "# Ensure NLTK resources are downloaded for sentence tokenization\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"Downloading 'punkt' package...\")\n",
        "    nltk.download('punkt')\n",
        "\n",
        "def get_useragent():\n",
        "    _useragent_list = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.62',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0'\n",
        "    ]\n",
        "    return random.choice(_useragent_list)\n",
        "\n",
        "def google_search(query):\n",
        "    \"\"\"Perform Google search and return the list of URLs.\"\"\"\n",
        "    links = []\n",
        "    try:\n",
        "        # Perform the Google search request\n",
        "        for url in search(query, lang='en'):\n",
        "            links.append(url)  # Append each URL to the links list\n",
        "    except Exception as e:\n",
        "        print(f\"Error during the search request: {e}\")\n",
        "    return links\n",
        "\n",
        "async def fetch_web_content(url: str, session: aiohttp.ClientSession) -> str:\n",
        "    \"\"\"Fetch and clean the main content of a webpage.\"\"\"\n",
        "    try:\n",
        "        headers = {\"User-Agent\": get_useragent()}\n",
        "        async with session.get(url, headers=headers, timeout=5) as response:\n",
        "            if response.status == 403:\n",
        "                print(f\"Access denied for {url}\")\n",
        "                return \"\"\n",
        "            content = await response.text(errors='ignore')\n",
        "            return content\n",
        "    except aiohttp.ClientError as e:\n",
        "        print(f\"Failed to fetch {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_shingles(text: str, k: int = 5) -> set:\n",
        "    \"\"\"Generate k-shingles (sets of k consecutive words) for a given text.\"\"\"\n",
        "    words = text.split()\n",
        "    shingles = set()\n",
        "    for i in range(len(words) - k + 1):\n",
        "        shingle = \" \".join(words[i:i + k])\n",
        "        shingle_hash = hashlib.md5(shingle.encode(\"utf-8\")).hexdigest()\n",
        "        shingles.add(shingle_hash)\n",
        "    return shingles\n",
        "\n",
        "def similarity1(set1, set2):\n",
        "    \"\"\"Calculate the similarity score based on intersection of shingles.\"\"\"\n",
        "    if len(set1) == 0 or len(set2) == 0:\n",
        "        return 0.0\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    return (intersection / len(set1)) * 100\n",
        "\n",
        "async def check_sentence_plagiarism(sentence: str) -> Dict[str, Any]:\n",
        "    \"\"\"Check plagiarism for a single sentence by searching and comparing content.\"\"\"\n",
        "    result = {\"sentence\": sentence, \"matches\": []}\n",
        "    urls = google_search(sentence)  # Call google_search synchronously\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [fetch_web_content(url, session) for url in urls]\n",
        "        contents = await asyncio.gather(*tasks)\n",
        "\n",
        "    for url, content in zip(urls, contents):\n",
        "        if content:\n",
        "            original_shingles = get_shingles(sentence)\n",
        "            content_shingles = get_shingles(content)\n",
        "            similarity = similarity1(original_shingles, content_shingles)\n",
        "            result[\"matches\"].append({\"url\": url, \"score\": similarity})\n",
        "\n",
        "    # Sort matches by score in descending order and take the highest\n",
        "    if result[\"matches\"]:\n",
        "        result[\"matches\"].sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "        highest_match = result[\"matches\"][0]\n",
        "        result[\"highest_match\"] = {\"url\": highest_match[\"url\"], \"score\": highest_match[\"score\"]}\n",
        "    else:\n",
        "        result[\"highest_match\"] = {\"url\": None, \"score\": 0.0}\n",
        "\n",
        "    return result\n",
        "\n",
        "async def plagiarism_checker(text: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Run plagiarism check on each sentence in the input text.\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    results = await asyncio.gather(*[check_sentence_plagiarism(sentence) for sentence in sentences])\n",
        "    return get_score(results)\n",
        "def get_score(data):\n",
        "    \"\"\"Generate a final plagiarism score, excluding scores less than 30.\"\"\"\n",
        "    result = []\n",
        "    total_score = 0\n",
        "    max_score = 0\n",
        "    max_score_url = \"\"\n",
        "    total_sentences = 0\n",
        "\n",
        "    for entry in data:\n",
        "        url = entry['highest_match']['url']\n",
        "        score = entry['highest_match']['score']\n",
        "        sentence = entry['sentence']\n",
        "\n",
        "        # Exclude sentences with score less than 30\n",
        "        if score < 30:\n",
        "            plagiarism = False  # Mark as non-plagiarized\n",
        "        else:\n",
        "            plagiarism = True  # Mark as plagiarized if score >= 30\n",
        "\n",
        "        # If the score is 30 or higher, add it to total_score for average calculation\n",
        "        if score >= 30:\n",
        "            total_score += score\n",
        "            total_sentences += 1  # Count only plagiarized sentences for average\n",
        "\n",
        "        # Track maximum score and corresponding URL\n",
        "        if score > max_score:\n",
        "            max_score = score\n",
        "            max_score_url = url\n",
        "\n",
        "        # Check if we should merge with the last entry\n",
        "        if result and result[-1]['url'] == url and result[-1]['score'] == score:\n",
        "            result[-1]['sentence'] += \" \" + sentence  # Append the sentence\n",
        "        else:\n",
        "            # Add a new entry with plagiarism flag\n",
        "            result.append({\n",
        "                'sentence': sentence,\n",
        "                'url': url,\n",
        "                'score': score,\n",
        "                'plagiarism': plagiarism\n",
        "            })\n",
        "\n",
        "    # Calculate average score only from sentences with score >= 30\n",
        "    average_score = total_score / total_sentences if total_sentences else 0\n",
        "\n",
        "    # Create the final structured response\n",
        "    response = {\n",
        "        \"plagiarism\": any(entry['plagiarism'] for entry in result),\n",
        "        \"average_score\": average_score,\n",
        "        \"max_score\": max_score,\n",
        "        \"max_score_url\": max_score_url,\n",
        "        \"data\": result\n",
        "    }\n",
        "\n",
        "    return response\n",
        "\n",
        "# Example input text for plagiarism checking\n",
        "input_text = \"\"\"\n",
        "An Operating System can be defined as an interface between user and hardware. It is responsible for the execution of all the processes, Resource Allocation, CPU scheduling, memory management, and providing security. The OS also manages devices such as printers, monitors, and storage devices. Without an operating system, a computer cannot function effectively.\n",
        "\"\"\"\n",
        "# Running the plagiarism checker asynchronously\n",
        "results_plagiarism = await plagiarism_checker(input_text)\n",
        "\n",
        "# Printing the results in a readable format\n",
        "print(json.dumps(results_plagiarism, indent=2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def detect_ai_generated_text_advanced_v3(\n",
        "    text,\n",
        "    model_name=\"roberta-base-openai-detector\",\n",
        "    threshold=0.5,\n",
        "    max_length=512\n",
        "):\n",
        "    \"\"\"\n",
        "    Advanced detection of AI-generated text with detailed metrics, AI flag, and percentage.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): Input text to analyze.\n",
        "        model_name (str): Pre-trained model to use for detection.\n",
        "        threshold (float): Confidence threshold for categorization (default: 0.5).\n",
        "        max_length (int): Maximum length for tokenization (default: 512).\n",
        "\n",
        "    Returns:\n",
        "        dict: Detailed output with categorization, confidence, metrics, AI flag, and percentage.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load tokenizer and model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "        # Tokenize and prepare text\n",
        "        tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "        token_count = len(tokens[\"input_ids\"][0])\n",
        "\n",
        "        # Inference\n",
        "        outputs = model(**tokens)\n",
        "        probabilities = torch.softmax(outputs.logits, dim=1).detach().numpy()[0]\n",
        "        ai_confidence = probabilities[1]\n",
        "        human_confidence = probabilities[0]\n",
        "        category = \"AI-generated\" if ai_confidence > threshold else \"Human-written\"\n",
        "\n",
        "        # AI flag and percentage\n",
        "        is_ai = ai_confidence > threshold\n",
        "        ai_percentage = round(ai_confidence * 100, 2)\n",
        "\n",
        "        # Additional metrics\n",
        "        char_count = len(text)\n",
        "        word_count = len(text.split())\n",
        "        avg_word_length = char_count / word_count if word_count > 0 else 0\n",
        "        confidence_diff = abs(ai_confidence - human_confidence)\n",
        "\n",
        "        return {\n",
        "            \"category\": category,\n",
        "            \"AI\": is_ai,\n",
        "            \"AI_Percentage\": ai_percentage,\n",
        "            \"confidence\": ai_confidence if is_ai else human_confidence,\n",
        "            \"confidence_difference\": confidence_diff,\n",
        "            \"probabilities\": {\n",
        "                \"Human-written\": human_confidence,\n",
        "                \"AI-generated\": ai_confidence\n",
        "            },\n",
        "            \"metrics\": {\n",
        "                \"character_count\": char_count,\n",
        "                \"word_count\": word_count,\n",
        "                \"average_word_length\": round(avg_word_length, 2),\n",
        "                \"token_count\": token_count,\n",
        "                \"max_token_length\": max_length\n",
        "            },\n",
        "            \"threshold_used\": threshold\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# Example usage\n",
        "#text = \"In a world driven by rapid technological advancement, artificial intelligence has emerged as a transformative force. From revolutionizing healthcare with predictive diagnostics to enhancing efficiency in industries through automation, AI continues to redefine the boundaries of what is possible. However, this unprecedented growth also poses ethical dilemmas, emphasizing the need for responsible innovation to ensure these technologies benefit humanity as a whole.\"\n",
        "#result_AI = detect_ai_generated_text_advanced_v3(text, threshold=0.7)\n",
        "#print(result)\n",
        "#print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "import math\n",
        "import torch\n",
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load model\n",
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if 'model' not in globals():\n",
        "    model = AutoAWQForCausalLM.from_quantized(\n",
        "        model_name_or_path,\n",
        "        fuse_layers=True,\n",
        "        trust_remote_code=False,\n",
        "        safetensors=True\n",
        "    ).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
        "\n",
        "models_loaded = True\n",
        "'''\n",
        "# Function to evaluate the student's answer\n",
        "def evaluate_answer(question, answer, full_marks=5, expected_answer=None):\n",
        "    # Basic Information\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Full Marks: {full_marks}\n",
        "    \"\"\"\n",
        "\n",
        "    if expected_answer:\n",
        "        prompt += f\"Expected Answer: {expected_answer}\\n\"\n",
        "\n",
        "    # Evaluation instructions\n",
        "    prompt += \"\"\"\n",
        "    Evaluate based on strictly: Correctness of answer, Completeness of answer, Clarity, Depth, and Relevance of answer , if answer not according to asked question or giving nearly similar answer just give it a 0 marks.\n",
        "\n",
        "    Return JSON with:\n",
        "    - marks_obtained\n",
        "    - feedback  ( detailed overall feedback including the grading evolution and marks the error like grammar mistakes or conceptual mistakes or the completeness mistakes along with the posible good answer)\n",
        "    - areas_for_improvemen ( what to write for getting full marks for this question)\n",
        "    - clarity_score (1-10)\n",
        "    - depth_score (1-10)\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the prompt for the model\n",
        "    tokens = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
        "\n",
        "    # Generate the model's response with the given prompt\n",
        "    generation_output = model.generate(\n",
        "        tokens, do_sample=True, temperature=0.7, top_p=0.95, top_k=40, max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Decode the output from the model\n",
        "    response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "'''\n",
        "\n",
        "def round_based_on_fraction(number):\n",
        "    # Get the fractional part of the number\n",
        "    fractional_part = number - math.floor(number)\n",
        "\n",
        "    # If the fractional part is greater than 0.5, round up, else round down\n",
        "    if fractional_part > 0.5:\n",
        "        return math.ceil(number)  # Round up\n",
        "    else:\n",
        "        return math.floor(number)  # Round down\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_score_and_feedback(generated_text):\n",
        "    \"\"\"\n",
        "    Extract detailed evaluation information from the generated text using regular expressions.\n",
        "\n",
        "    :param generated_text: The raw output from the model.\n",
        "    :return: A dictionary with extracted evaluation details.\n",
        "    \"\"\"\n",
        "    # Define the patterns for each field\n",
        "    score_pattern = r'\"marks_obtained\":\\s*([0-9.]+)'  # Extract numerical score\n",
        "    feedback_pattern = r'\"feedback\":\\s*\"([^\"]+)\"'  # Extract feedback text, handling newlines\n",
        "    strengths_pattern = r'\"strengths\":\\s*\\[([^\\]]+)\\]'  # Extract strengths list\n",
        "    areas_for_improvement_pattern = r'\"areas_for_improvement\":\\s*\\[([^\\]]+)\\]'  # Extract areas for improvement list\n",
        "    clarity_score_pattern = r'\"clarity_score\":\\s*([0-9.]+)'  # Extract clarity score\n",
        "    depth_score_pattern = r'\"depth_score\":\\s*([0-9.]+)'  # Extract depth score\n",
        "\n",
        "    # Find matches using regular expressions\n",
        "    score_match = re.search(score_pattern, generated_text)\n",
        "    feedback_match = re.search(feedback_pattern, generated_text)\n",
        "    strengths_match = re.search(strengths_pattern, generated_text)\n",
        "    areas_for_improvement_match = re.search(areas_for_improvement_pattern, generated_text)\n",
        "    clarity_score_match = re.search(clarity_score_pattern, generated_text)\n",
        "    depth_score_match = re.search(depth_score_pattern, generated_text)\n",
        "\n",
        "    # Extract values or set them to None if not found\n",
        "    score = float(score_match.group(1)) if score_match else None\n",
        "    feedback = feedback_match.group(1).strip() if feedback_match else None  # Strip to remove extra spaces/newlines\n",
        "\n",
        "    # Handle strengths and areas for improvement (could be a list or a single string)\n",
        "    strengths = parse_list_or_string(strengths_match.group(1)) if strengths_match else []\n",
        "    areas_for_improvement = parse_list_or_string(areas_for_improvement_match.group(1)) if areas_for_improvement_match else []\n",
        "\n",
        "    clarity_score = float(clarity_score_match.group(1)) if clarity_score_match else None\n",
        "    depth_score = float(depth_score_match.group(1)) if depth_score_match else None\n",
        "\n",
        "    # Return the extracted data as a dictionary\n",
        "    return {\n",
        "        \"marks_obtained\": score,\n",
        "        \"feedback\": feedback,\n",
        "        \"strengths\": strengths,\n",
        "        \"areas_for_improvement\": areas_for_improvement,\n",
        "        \"clarity_score\": clarity_score,\n",
        "        \"depth_score\": depth_score\n",
        "    }\n",
        "\n",
        "def parse_list_or_string(value):\n",
        "    \"\"\"\n",
        "    Parse a field that could either be a list of strings or a single string.\n",
        "    If it's a list, it will return a list; if it's a single string, it will return a list with one item.\n",
        "\n",
        "    :param value: The raw string to be parsed (could be a list or a string).\n",
        "    :return: A list of strings.\n",
        "    \"\"\"\n",
        "    # If the value looks like a list (contains commas), split it into a list\n",
        "    if isinstance(value, str) and \",\" in value:\n",
        "        # Remove leading/trailing whitespace and split by commas\n",
        "        return [item.strip().strip('\"') for item in value.split(\",\")]\n",
        "    # Otherwise, return a list containing the value as a single item\n",
        "    elif isinstance(value, str):\n",
        "        return [value.strip().strip('\"')]\n",
        "    return []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import asyncio\n",
        "import math\n",
        "import re\n",
        "\n",
        "async def GetData(question, student_answer, expected_answer, marks):\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Ensure non-empty inputs\n",
        "    student_answer = student_answer or \"\"  # Default to empty string if None or empty\n",
        "    expected_answer = expected_answer or \"\"  # Default to empty string if None or empty\n",
        "    marks = marks or 0  # Default to 0 if marks are None or empty\n",
        "    question = question or \"\"  # Default to empty string if None or empty\n",
        "\n",
        "    # Generate response using the model\n",
        "    generated_text = evaluate_answer(question, student_answer, marks, expected_answer)\n",
        "\n",
        "    # Extract score and feedback from the model's output\n",
        "    result = extract_score_and_feedback(generated_text)\n",
        "\n",
        "    # Handle missing or None values gracefully using .get() with default values\n",
        "    score = result.get('marks_obtained', 0)  # Default to 0 if score is None or not found\n",
        "    score_old = score\n",
        "    feedback = result.get('feedback', \"\")  # Default to an empty string if feedback is None or not found\n",
        "    strengths = result.get('strengths', \"\")  # Default to an empty string if strengths are None or not found\n",
        "    areas_for_improvement = result.get('areas_for_improvement', \"\")  # Default to an empty string if areas are None or not found\n",
        "    clarity_score = result.get('clarity_score', 0)  # Default to 0 if clarity score is None or not found\n",
        "    depth_score = result.get('depth_score', 0)  # Default to 0 if depth score is None or not found\n",
        "\n",
        "    # Ensure score is not None before using it in arithmetic operations\n",
        "    score = score if score is not None else 0\n",
        "\n",
        "    # Logarithmic scaling of score\n",
        "    log_score = math.log(score + 1) if score is not None else 0  # Logarithm of score (score + 1 to avoid log(0))\n",
        "    weighted_score = log_score * 0.5  # Adjust weight as needed\n",
        "\n",
        "    # Calculate initial marks based on the weighted sum\n",
        "    final_marks_obtained = weighted_score * marks  # Scale by full marks\n",
        "\n",
        "    # Initialize penalty and reason\n",
        "    penalty = 0.0\n",
        "    penalty_reasons = []\n",
        "\n",
        "    # Apply penalty if marks exceed 90% of full marks\n",
        "    max_allowed_marks = marks * 0.9\n",
        "    if final_marks_obtained > max_allowed_marks:\n",
        "        penalty_factor = 0.8  # Apply a 20% penalty\n",
        "        final_marks_obtained *= penalty_factor\n",
        "        penalty += 0.2  # Penalty is 20% of the score\n",
        "        penalty_reasons.append(\"Penalty applied for exceeding 90% of full marks\")\n",
        "\n",
        "    # Check plagiarism and AI detection\n",
        "    results_plagiarism = await plagiarism_checker(student_answer) or {}\n",
        "    result_AI = detect_ai_generated_text_advanced_v3(student_answer, threshold=0.7) or {}\n",
        "\n",
        "    if results_plagiarism.get(\"plagiarism\", False) and results_plagiarism.get(\"average_score\", 0.0) > 0.3:\n",
        "        penalty += 0.25  # Apply a 25% penalty for plagiarism\n",
        "        penalty_reasons.append(\"Penalty applied for plagiarism exceeding 30%\")\n",
        "\n",
        "    if result_AI.get(\"AI\", False) and result_AI.get(\"AI_Percentage\", 0.0) > 30:\n",
        "        penalty += 0.25  # Apply a 25% penalty for AI-generated text exceeding 30%\n",
        "        penalty_reasons.append(\"Penalty applied for AI detection exceeding 30%\")\n",
        "\n",
        "    # Apply total penalty and adjust marks\n",
        "    final_marks_obtained = final_marks_obtained * (1 - penalty)\n",
        "\n",
        "    # Ensure marks are never below 0\n",
        "    final_marks_obtained = max(final_marks_obtained, 0)\n",
        "\n",
        "    # Analyze student's answer\n",
        "    student_word_count = len(student_answer.split())  # Count words in student's answer\n",
        "    student_sentence_count = len(re.split(r'[.!?]+', student_answer.strip())) - 1  # Count sentences\n",
        "\n",
        "    # Prepare final result dictionary\n",
        "    result_data = {\n",
        "        \"full_marks\": marks,\n",
        "        \"obtained_marks\": score_old,\n",
        "        \"strengths\": strengths,\n",
        "        \"areas_for_improvement\": areas_for_improvement,\n",
        "        \"clarity_score\": clarity_score,\n",
        "        \"depth_score\": depth_score,\n",
        "        \"final_marks_obtained\": round_based_on_fraction(final_marks_obtained),\n",
        "        \"feedback\": feedback,\n",
        "        \"student_word_count\": student_word_count,\n",
        "        \"student_sentence_count\": student_sentence_count,\n",
        "        \"plagiarism\": results_plagiarism.get(\"plagiarism\", False),\n",
        "        \"average_score\": results_plagiarism.get(\"average_score\", 0.0),\n",
        "        \"max_score\": results_plagiarism.get(\"max_score\", 0.0),\n",
        "        \"max_score_url\": results_plagiarism.get(\"max_score_url\", \"\"),\n",
        "        \"AI_category\": result_AI.get(\"category\", \"Unknown\"),\n",
        "        \"AI_flag\": result_AI.get(\"AI\", False),\n",
        "        \"AI_percentage\": result_AI.get(\"AI_Percentage\", 0.0),\n",
        "        \"AI_probabilities\": result_AI.get(\"probabilities\", {}),\n",
        "        \"reason\": \"; \".join(penalty_reasons),  # List of reasons for penalties applied\n",
        "    }\n",
        "\n",
        "    return result_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import asyncio\n",
        "import math\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to evaluate the student's answer\n",
        "def evaluate_answer(question, answer, full_marks=5, expected_answer=None):\n",
        "    # Basic Information\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Full Marks: {full_marks}\n",
        "    \"\"\"\n",
        "\n",
        "    if expected_answer:\n",
        "        prompt += f\"Expected Answer: {expected_answer}\\n\"\n",
        "\n",
        "    # Evaluation instructions\n",
        "    prompt += \"\"\"\n",
        "    Evaluate based on strictly: Correctness of answer, Completeness of answer, Clarity, Depth, and Relevance of answer , if answer not according to asked question or giving nearly similar answer just give it a 0 marks.\n",
        "\n",
        "    Return JSON with:\n",
        "    - marks_obtained\n",
        "    - feedback  ( brief compact overall feedback including the grading evolution and marks the error like grammar mistakes or conceptual mistakes or the completeness mistakes along with the posible good answer)\n",
        "    - areas_for_improvemen ( what to write for getting full marks for this question)\n",
        "    - clarity_score (1-10)\n",
        "    - depth_score (1-10)\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the prompt for the model\n",
        "    tokens = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
        "\n",
        "    # Generate the model's response with the given prompt\n",
        "    generation_output = model.generate(\n",
        "        tokens, do_sample=True, temperature=0.7, top_p=0.95, top_k=40, max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Decode the output from the model\n",
        "    response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "async def GetData(question, student_answer, expected_answer, marks):\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Ensure non-empty inputs\n",
        "    student_answer = student_answer or \"\"  # Default to empty string if None or empty\n",
        "    expected_answer = expected_answer or \"\"  # Default to empty string if None or empty\n",
        "    marks = marks or 0  # Default to 0 if marks are None or empty\n",
        "    question = question or \"\"  # Default to empty string if None or empty\n",
        "\n",
        "    # Generate response using the model\n",
        "    generated_text = evaluate_answer(question, student_answer, marks, expected_answer)\n",
        "\n",
        "    # Extract score and feedback from the model's output\n",
        "    result = extract_score_and_feedback(generated_text)\n",
        "\n",
        "    # Handle missing or None values gracefully using .get() with default values\n",
        "    score = result.get('marks_obtained', 0)  # Default to 0 if score is None or not found\n",
        "    score_old = score\n",
        "    feedback = result.get('feedback', \"\")  # Default to an empty string if feedback is None or not found\n",
        "    strengths = result.get('strengths', \"\")  # Default to an empty string if strengths are None or not found\n",
        "    areas_for_improvement = result.get('areas_for_improvement', \"\")  # Default to an empty string if areas are None or not found\n",
        "    clarity_score = result.get('clarity_score', 0)  # Default to 0 if clarity score is None or not found\n",
        "    depth_score = result.get('depth_score', 0)  # Default to 0 if depth score is None or not found\n",
        "\n",
        "    # Ensure score is not None before using it in arithmetic operations\n",
        "    score = score if score is not None else 0\n",
        "\n",
        "    # Logarithmic scaling of score\n",
        "    log_score = math.log(score + 1) if score is not None else 0  # Logarithm of score (score + 1 to avoid log(0))\n",
        "    weighted_score = log_score * 0.5  # Adjust weight as needed\n",
        "\n",
        "    # Calculate initial marks based on the weighted sum\n",
        "    final_marks_obtained = weighted_score * marks  # Scale by full marks\n",
        "\n",
        "    # Initialize penalty and reason\n",
        "    penalty = 0.0\n",
        "    penalty_reasons = []\n",
        "\n",
        "    # Apply penalty if marks exceed 90% of full marks\n",
        "    max_allowed_marks = marks * 0.9\n",
        "    if final_marks_obtained > max_allowed_marks:\n",
        "        penalty_factor = 0.8  # Apply a 20% penalty\n",
        "        final_marks_obtained *= penalty_factor\n",
        "        penalty += 0.2  # Penalty is 20% of the score\n",
        "        penalty_reasons.append(\"Penalty applied for exceeding 90% of full marks\")\n",
        "\n",
        "    # Check plagiarism and AI detection\n",
        "    results_plagiarism = await plagiarism_checker(student_answer) or {}\n",
        "    result_AI = detect_ai_generated_text_advanced_v3(student_answer, threshold=0.7) or {}\n",
        "\n",
        "    if results_plagiarism.get(\"plagiarism\", False) and results_plagiarism.get(\"average_score\", 0.0) > 0.3:\n",
        "        penalty += 0.25  # Apply a 25% penalty for plagiarism\n",
        "        penalty_reasons.append(\"Penalty applied for plagiarism exceeding 30%\")\n",
        "\n",
        "    if result_AI.get(\"AI\", False) and result_AI.get(\"AI_Percentage\", 0.0) > 30:\n",
        "        penalty += 0.25  # Apply a 25% penalty for AI-generated text exceeding 30%\n",
        "        penalty_reasons.append(\"Penalty applied for AI detection exceeding 30%\")\n",
        "\n",
        "    # Apply total penalty and adjust marks\n",
        "    final_marks_obtained = final_marks_obtained * (1 - penalty)\n",
        "\n",
        "    # Ensure marks are never below 0\n",
        "    final_marks_obtained = max(final_marks_obtained, 0)\n",
        "\n",
        "    # Analyze student's answer\n",
        "    student_word_count = len(student_answer.split())  # Count words in student's answer\n",
        "    student_sentence_count = len(re.split(r'[.!?]+', student_answer.strip())) - 1  # Count sentences\n",
        "\n",
        "    # Prepare final result dictionary\n",
        "    result_data = {\n",
        "        \"full_marks\": marks,\n",
        "        \"obtained_marks\": score_old,\n",
        "        \"strengths\": strengths,\n",
        "        \"areas_for_improvement\": areas_for_improvement,\n",
        "        \"clarity_score\": clarity_score,\n",
        "        \"depth_score\": depth_score,\n",
        "        \"final_marks_obtained\": round_based_on_fraction(final_marks_obtained),\n",
        "        \"feedback\": feedback,\n",
        "        \"student_word_count\": student_word_count,\n",
        "        \"student_sentence_count\": student_sentence_count,\n",
        "        \"plagiarism\": results_plagiarism.get(\"plagiarism\", False),\n",
        "        \"average_score\": results_plagiarism.get(\"average_score\", 0.0),\n",
        "        \"max_score\": results_plagiarism.get(\"max_score\", 0.0),\n",
        "        \"max_score_url\": results_plagiarism.get(\"max_score_url\", \"\"),\n",
        "        \"AI_category\": result_AI.get(\"category\", \"Unknown\"),\n",
        "        \"AI_flag\": result_AI.get(\"AI\", False),\n",
        "        \"AI_percentage\": result_AI.get(\"AI_Percentage\", 0.0),\n",
        "        \"AI_probabilities\": result_AI.get(\"probabilities\", {}),\n",
        "        \"reason\": \"; \".join(penalty_reasons),  # List of reasons for penalties applied\n",
        "    }\n",
        "\n",
        "    return result_data"
      ],
      "metadata": {
        "id": "AQj20Y0kLuPy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "ZJJsCuhHG82u",
        "outputId": "f1c146ba-099c-4791-93e4-36901e75ed61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.13.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.7-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.6.0 (from gradio)\n",
            "  Downloading gradio_client-1.6.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.27.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.5)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.9.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.6.0->gradio) (2024.9.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.6.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.13.1-py3-none-any.whl (57.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.6.0-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.7-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.7 ffmpy-0.5.0 gradio-5.13.1 gradio-client-1.6.0 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.3 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "markupsafe"
                ]
              },
              "id": "d6a779bc95b141a3a29a2fbf09d50444"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gradio as gr\n",
        "import traceback\n",
        "import asyncio\n",
        "\n",
        "# The evaluation function should be async to handle the await call\n",
        "async def evaluate_answer_function(question, answer, full_marks):\n",
        "    try:\n",
        "        # Await the result from the async GetData function\n",
        "        result = await GetData(question, answer, \" \", full_marks)\n",
        "        return result  # Return the result directly (no JSON)\n",
        "    except Exception as e:\n",
        "        # Capture and return full error traceback for better debugging\n",
        "        error_message = traceback.format_exc()\n",
        "        return {\"error\": error_message}\n",
        "\n",
        "# Create Gradio Blocks Interface\n",
        "with gr.Blocks() as iface:\n",
        "    question = gr.Textbox(label=\"Question\", placeholder=\"Enter the exam question here\")\n",
        "    answer = gr.Textbox(label=\"Answer\", placeholder=\"Enter the student's answer here\")\n",
        "    full_marks = gr.Slider(minimum=0, maximum=100, label=\"Full Marks\", step=1)\n",
        "\n",
        "    output = gr.JSON(label=\"Evaluation Result\")\n",
        "\n",
        "    # Add a submit button that will trigger the evaluation\n",
        "    submit_button = gr.Button(\"Submit\")\n",
        "\n",
        "    # Connect the function to the button click event\n",
        "    submit_button.click(evaluate_answer_function, [question, answer, full_marks], output)\n",
        "\n",
        "# Launch the interface and expose API\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "id": "0BlprxrIG31j",
        "outputId": "2a16a8c4-e092-4d37-e741-7e064d635783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://be9316535b6ef53975.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-18d696619cc3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Launch the interface and expose API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0miface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, _frontend)\u001b[0m\n\u001b[1;32m   2697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_url\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2699\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnetworking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl_ok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshare_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2700\u001b[0m                         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2701\u001b[0m                     artifact = HTML(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/networking.py\u001b[0m in \u001b[0;36murl_ok\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             if (\n\u001b[1;32m     58\u001b[0m                 \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m401\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m302\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m303\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m307\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_api.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(url, params, headers, cookies, auth, proxy, follow_redirects, verify, timeout, trust_env)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mon\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mHEAD\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mrequests\u001b[0m \u001b[0mshould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minclude\u001b[0m \u001b[0ma\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \"\"\"\n\u001b[0;32m--> 267\u001b[0;31m     return request(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, params, content, data, files, json, headers, cookies, auth, proxy, timeout, follow_redirects, verify, trust_env)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mtrust_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     ) as client:\n\u001b[0;32m--> 109\u001b[0;31m         return client.request(\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         )\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect_failed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0mssl_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_extra_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ssl_object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    122\u001b[0m                     }\n\u001b[1;32m    123\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"connect_tcp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_tcp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                         \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mconnect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             sock = socket.create_connection(\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask pyngrok"
      ],
      "metadata": {
        "id": "DkEkte2HVQV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from flask import Flask, request\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "\n",
        "# API route to handle GET requests\n",
        "@app.route(\"/\", methods=[\"GET\"])\n",
        "def evaluate_answer():\n",
        "    try:\n",
        "        # Get parameters from the GET request\n",
        "        question = request.args.get(\"question\")\n",
        "        answer = request.args.get(\"answer\")\n",
        "        marks = request.args.get(\"marks\")\n",
        "\n",
        "        if not question or not answer or not marks:\n",
        "            return f\"Error: Missing required parameters (question, answer, marks)\", 400\n",
        "\n",
        "        # Call GetData function to evaluate\n",
        "        result = GetData(question, answer, \" \", marks)\n",
        "\n",
        "        # Return the result directly as a string or dictionary\n",
        "        return str(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\", 500\n",
        "\n",
        "# Run the Flask app\n",
        "if __name__ == \"__main__\":\n",
        "    # Expose the Flask app on port 8000\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(f\"Flask app is running on: {public_url}\")\n",
        "\n",
        "    # Start Flask app\n",
        "    app.run(port=8000)"
      ],
      "metadata": {
        "id": "3pAl4LbtSffz",
        "outputId": "f543b46f-d8fb-44e9-efbe-3c1cff00f159",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask app is running on: NgrokTunnel: \"https://cf8f-34-124-165-219.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:8000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jan/2025 06:41:23] \"\u001b[31m\u001b[1mGET / HTTP/1.1\u001b[0m\" 400 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jan/2025 06:41:24] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "/usr/local/lib/python3.11/dist-packages/flask/app.py:902: RuntimeWarning: coroutine 'GetData' was never awaited\n",
            "  return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jan/2025 06:41:54] \"GET /?question=What%20is%20an%20Operating%20System?&answer=An%20OS%20manages%20hardware%20and%20software%20resources.&marks=10 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jan/2025 06:41:57] \"GET /?question=What+is+an+Operating+System?&answer=An+OS+manages+hardware+and+software+resources.&marks=10 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jan/2025 06:41:58] \"GET /?question=What+is+an+Operating+System?&answer=An+OS+manages+hardware+and+software+resources.&marks=10 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jan/2025 06:41:58] \"GET /?question=What+is+an+Operating+System?&answer=An+OS+manages+hardware+and+software+resources.&marks=10 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jan/2025 06:43:52] \"GET /?question=What%20is%20an%20Operating%20System?&answer=An%20OS%20manages%20hardware%20and%20software%20resources.&marks=10 HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jan/2025 06:48:23] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Jan/2025 06:49:03] \"\u001b[31m\u001b[1mGET / HTTP/1.1\u001b[0m\" 400 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from flask import Flask, request, jsonify\n",
        "import asyncio\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "\n",
        "@app.route('/', methods=['GET'])\n",
        "async def evaluate_answer():\n",
        "    question = request.args.get('question')\n",
        "    answer = request.args.get('answer')\n",
        "    marks = request.args.get('marks')\n",
        "\n",
        "    # Await the async function GetData\n",
        "    result = await GetData(question, answer, \" \", marks)\n",
        "\n",
        "    # Return the result as JSON\n",
        "    return jsonify(result)\n",
        "\n",
        "# Start the Flask app and use ngrok to expose it\n",
        "if __name__ == \"__main__\":\n",
        "    # Set up ngrok tunnel\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(f\"Flask app is running on: {public_url}\")\n",
        "\n",
        "    # Run the Flask app\n",
        "    app.run(debug=True, host='0.0.0.0', port=8000)"
      ],
      "metadata": {
        "id": "rKHaaK9EYCPp",
        "outputId": "17525a38-b3ee-4516-cef8-23014e383ec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flask app is running on: NgrokTunnel: \"https://dc1f-34-124-165-219.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:8000\n",
            " * Running on http://172.28.0.12:8000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}