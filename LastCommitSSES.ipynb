{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfMnf4GOIgSjoJYcLiZI4p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeveloperPratim/sses_final/blob/main/LastCommitSSES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCphRmbu9SVx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util  # For semantic similarityimport math\n",
        "\n",
        "# Global variables for models and tokenizers\n",
        "model_qwen = None\n",
        "tokenizer_qwen = None\n",
        "sentence_transformer = None\n",
        "models_loaded = False  # Flag to ensure models are loaded only once\n",
        "\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"\n",
        "    Load global models and tokenizers.\n",
        "    This function should be called only once.\n",
        "    \"\"\"\n",
        "    global model_qwen, tokenizer_qwen, sentence_transformer, models_loaded\n",
        "\n",
        "    if not models_loaded:  # Load models only if not already loaded\n",
        "        print(\"Loading models...\")\n",
        "\n",
        "        # Load Qwen model\n",
        "        drive_path_qwen = \"/content/drive/MyDrive/sses/Qwen2.5-1.5B-Instruct\"\n",
        "        tokenizer_qwen = AutoTokenizer.from_pretrained(drive_path_qwen, local_files_only=True)\n",
        "        model_qwen = AutoModelForCausalLM.from_pretrained(drive_path_qwen, local_files_only=True).to(torch.device(\"cpu\"))\n",
        "\n",
        "        # Load SentenceTransformer for semantic similarity (MiniLM model)\n",
        "        drive_path_minilm = \"/content/drive/MyDrive/sses/all-MiniLM-L6-v2\"\n",
        "        sentence_transformer = SentenceTransformer(drive_path_minilm, device=\"cpu\")\n",
        "\n",
        "        models_loaded = True\n",
        "        print(\"Models loaded successfully.\")\n",
        "    else:\n",
        "        print(\"Models are already loaded.\")\n",
        "\n",
        "\n",
        "def generate_response(question, student_answer, marks):\n",
        "    \"\"\"\n",
        "    Generate a response using the Qwen model.\n",
        "\n",
        "    :param question: The input question.\n",
        "    :param student_answer: The input student's answer.\n",
        "    :return: The generated text response.\n",
        "    \"\"\"\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Prepare the input text\n",
        "    input_text = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {student_answer}\n",
        "    Evaluate and provide these two fields **score** and **feedback** based on this answer where full marks is {marks} and can be given for the most precise and best answer, and 0 for wrong or irrelevant answers. Respond only with these two fields in JSON format.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    input_ids = tokenizer_qwen.encode(input_text, return_tensors=\"pt\").to(torch.device(\"cpu\"))\n",
        "\n",
        "    # Dynamically set the maximum number of new tokens\n",
        "    max_allowed_tokens = model_qwen.config.max_position_embeddings  # Total token limit of the model\n",
        "    input_token_count = len(input_ids[0])\n",
        "    max_new_tokens = max(50, max_allowed_tokens - input_token_count)  # Minimum 50 new tokens\n",
        "\n",
        "    # Generate the response\n",
        "    output_ids = model_qwen.generate(\n",
        "        input_ids,\n",
        "        max_length=input_token_count + max_new_tokens,  # Total length: input + generated tokens\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,  # Avoid repetition\n",
        "        early_stopping=True,    # Stop when the output is complete\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated output\n",
        "    return tokenizer_qwen.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def extract_score_and_feedback(generated_text):\n",
        "    \"\"\"\n",
        "    Extract score and feedback from the generated text using regular expressions.\n",
        "\n",
        "    :param generated_text: The raw output from the model.\n",
        "    :return: A dictionary with \"score\" and \"feedback\".\n",
        "    \"\"\"\n",
        "    score_pattern = r'\"score\":\\s*([0-9.]+)'  # Matches numeric values for score\n",
        "    feedback_pattern = r'\"feedback\":\\s*\"([^\"]+)\"'  # Matches the feedback string\n",
        "\n",
        "    # Find matches\n",
        "    score_match = re.search(score_pattern, generated_text)\n",
        "    feedback_match = re.search(feedback_pattern, generated_text)\n",
        "\n",
        "    # Extract values\n",
        "    score = float(score_match.group(1)) if score_match else None  # Convert to float if match found\n",
        "    feedback = feedback_match.group(1) if feedback_match else None  # Extract feedback text if match found\n",
        "\n",
        "    return {\"score\": score, \"feedback\": feedback}\n",
        "\n",
        "\n",
        "def calculate_semantic_similarity(student_answer, expected_answer):\n",
        "    \"\"\"\n",
        "    Calculate the semantic similarity between the student's answer and the expected answer.\n",
        "\n",
        "    :param student_answer: The student's answer.\n",
        "    :param expected_answer: The expected correct answer.\n",
        "    :return: The semantic similarity score.\n",
        "    \"\"\"\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Encode both answers using the SentenceTransformer model\n",
        "    embeddings = sentence_transformer.encode([student_answer, expected_answer], convert_to_tensor=True)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity_score = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
        "    return similarity_score\n",
        "\n",
        "\n",
        "def GetData(question, student_answer, expected_answer, marks):\n",
        "    \"\"\"\n",
        "    Function to generate evaluation data based on the student's answer.\n",
        "\n",
        "    :param question: The question asked in the exam.\n",
        "    :param student_answer: The student's answer to the question.\n",
        "    :param expected_answer: The expected correct answer for the question.\n",
        "    :param marks: The full marks assigned to the question.\n",
        "\n",
        "    :return: A dictionary containing the analysis result.\n",
        "    \"\"\"\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Generate response using Qwen model\n",
        "    generated_text = generate_response(question, student_answer, marks)\n",
        "    # Extract score and feedback\n",
        "    result = extract_score_and_feedback(generated_text)\n",
        "    score = result['score']\n",
        "    feedback = result['feedback']\n",
        "\n",
        "    # Calculate semantic similarity between student's answer and expected answer\n",
        "    similarity = calculate_semantic_similarity(student_answer, expected_answer)\n",
        "\n",
        "    # Calculate final marks obtained based on score and semantic similarity\n",
        "    similarity_weight = 0.5  # You can adjust this weight based on your preference\n",
        "    final_marks_obtained = (score * 0.5) + (similarity * similarity_weight)\n",
        "\n",
        "    # Ensure that final_marks_obtained does not exceed the total marks\n",
        "    final_marks_obtained = min(final_marks_obtained, marks)\n",
        "\n",
        "    # Return the result as a dictionary\n",
        "    result_data = {\n",
        "        \"question\": question,\n",
        "        \"student_answer\": student_answer,\n",
        "        \"expected_answer\": expected_answer,\n",
        "        \"marks\": marks,\n",
        "        \"score\": score,\n",
        "        \"feedback\": feedback,\n",
        "        \"similarity\": similarity,\n",
        "        \"final_marks_obtained\": final_marks_obtained\n",
        "    }\n",
        "\n",
        "    return result_data\n",
        "\n",
        "\n",
        "def GetData(question, student_answer, expected_answer, marks):\n",
        "    \"\"\"\n",
        "    Function to generate evaluation data based on the student's answer.\n",
        "\n",
        "    :param question: The question asked in the exam.\n",
        "    :param student_answer: The student's answer to the question.\n",
        "    :param expected_answer: The expected correct answer for the question.\n",
        "    :param marks: The full marks assigned to the question.\n",
        "\n",
        "    :return: A dictionary containing the analysis result.\n",
        "    \"\"\"\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Generate response using Qwen model\n",
        "    generated_text = generate_response(question, student_answer, marks)\n",
        "    # Extract score and feedback\n",
        "    result = extract_score_and_feedback(generated_text)\n",
        "    score = result['score']\n",
        "    feedback = result['feedback']\n",
        "\n",
        "    # Calculate semantic similarity between student's answer and expected answer\n",
        "    similarity = calculate_semantic_similarity(student_answer, expected_answer)\n",
        "    similarity_percentage = round(similarity * 100, 2)  # Convert similarity to percentage\n",
        "\n",
        "    # Logarithmic scaling of score and similarity\n",
        "    log_score = math.log(score + 1)  # Logarithm of score (score + 1 to avoid log(0))\n",
        "    log_similarity = math.log(similarity + 1)  # Logarithm of similarity (similarity + 1 to avoid log(0))\n",
        "\n",
        "    # Weighted sum of score and similarity\n",
        "    weighted_score = log_score * 0.5  # Adjust weight as needed\n",
        "    weighted_similarity = log_similarity * 0.5  # Adjust weight as needed\n",
        "\n",
        "    # Calculate an initial final marks based on the weighted sum\n",
        "    final_marks_obtained = weighted_score + weighted_similarity\n",
        "    final_marks_obtained = final_marks_obtained * marks  # Scale by full marks\n",
        "\n",
        "    # Apply penalty: if the final marks exceed 90% of full marks, reduce them\n",
        "    max_allowed_marks = marks * 0.9  # No student should get more than 90% of full marks\n",
        "    if final_marks_obtained > max_allowed_marks:\n",
        "        penalty_factor = 0.8  # Apply a 20% penalty\n",
        "        final_marks_obtained = final_marks_obtained * penalty_factor\n",
        "\n",
        "    # Ensure that the final marks are never below 0\n",
        "    final_marks_obtained = max(final_marks_obtained, 0)\n",
        "\n",
        "    final_marks_obtained = ( final_marks_obtained  + score) /2\n",
        "\n",
        "\n",
        "    # Calculate percentages\n",
        "    final_marks_percentage = round((final_marks_obtained / marks) * 100, 2) if marks > 0 else 0\n",
        "    score_percentage = round((score / marks) * 100, 2) if marks > 0 else 0\n",
        "\n",
        "    # Analyze the student's answer\n",
        "    student_word_count = len(student_answer.split())  # Count words in the student's answer\n",
        "    student_sentence_count = len(re.split(r'[.!?]+', student_answer.strip())) - 1  # Count sentences\n",
        "\n",
        "    # Return the result as a dictionary\n",
        "    result_data = {\n",
        "        \"question\": question,\n",
        "        \"student_answer\": student_answer,\n",
        "        \"expected_answer\": expected_answer,\n",
        "        \"marks\": marks,\n",
        "        \"score\": score,\n",
        "        \"score_percentage\": score_percentage,\n",
        "        \"feedback\": feedback,\n",
        "        \"similarity\": similarity,\n",
        "        \"similarity_percentage\": similarity_percentage,\n",
        "        \"final_marks_obtained\": final_marks_obtained,\n",
        "        \"final_marks_percentage\": final_marks_percentage,\n",
        "        \"student_word_count\": student_word_count,\n",
        "        \"student_sentence_count\": student_sentence_count\n",
        "    }\n",
        "\n",
        "    return result_data\n",
        "\n",
        "# load models for first time\n",
        "load_models()  # Load models once"
      ],
      "metadata": {
        "id": "DULYrsEr9Vp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the purpose of a file system?\"\n",
        "student_answer = \"A file system manages files on storage devices and provides an interface for storing, retrieving, and organizing files.\"\n",
        "expected_answer = \"A file system organizes and controls how data is stored and retrieved on a computer system.\"\n",
        "marks = 5  # Full marks for the question\n",
        "\n",
        "# Get the evaluation data\n",
        "evaluation_result = GetData(question, student_answer, expected_answer, marks)\n",
        "print(evaluation_result)"
      ],
      "metadata": {
        "id": "rCqadeHg9vhF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}