{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeveloperPratim/sses_final/blob/main/Fixed%20GetData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O2V2AjcA_3A",
        "outputId": "29135ced-0bdc-4301-cdf0-137546439f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCphRmbu9SVx",
        "outputId": "b04d2eac-a820-4135-f1fb-6ea02b6539c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DULYrsEr9Vp1",
        "outputId": "1159afd5-085d-4566-9df8-eef6bd535a4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models...\n",
            "Models loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import math\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util  # For semantic similarityimport math\n",
        "\n",
        "# Global variables for models and tokenizers\n",
        "model_qwen = None\n",
        "tokenizer_qwen = None\n",
        "sentence_transformer = None\n",
        "models_loaded = False  # Flag to ensure models are loaded only once\n",
        "\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"\n",
        "    Load global models and tokenizers.\n",
        "    This function should be called only once.\n",
        "    \"\"\"\n",
        "    global model_qwen, tokenizer_qwen, sentence_transformer, models_loaded\n",
        "\n",
        "    if not models_loaded:  # Load models only if not already loaded\n",
        "        print(\"Loading models...\")\n",
        "\n",
        "        # Load Qwen model\n",
        "        drive_path_qwen = \"/content/drive/MyDrive/sses/Qwen2.5-1.5B-Instruct\"\n",
        "        tokenizer_qwen = AutoTokenizer.from_pretrained(drive_path_qwen, local_files_only=True)\n",
        "        model_qwen = AutoModelForCausalLM.from_pretrained(drive_path_qwen, local_files_only=True).to(torch.device(\"cpu\"))\n",
        "\n",
        "        # Load SentenceTransformer for semantic similarity (MiniLM model)\n",
        "        drive_path_minilm = \"/content/drive/MyDrive/sses/all-MiniLM-L6-v2\"\n",
        "        sentence_transformer = SentenceTransformer(drive_path_minilm, device=\"cpu\")\n",
        "\n",
        "        models_loaded = True\n",
        "        print(\"Models loaded successfully.\")\n",
        "    else:\n",
        "        print(\"Models are already loaded.\")\n",
        "\n",
        "\n",
        "def generate_response(question, student_answer, marks):\n",
        "    \"\"\"\n",
        "    Generate a response using the Qwen model.\n",
        "\n",
        "    :param question: The input question.\n",
        "    :param student_answer: The input student's answer.\n",
        "    :return: The generated text response.\n",
        "    \"\"\"\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Prepare the input text\n",
        "    input_text = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {student_answer}\n",
        "    Evaluate and provide these two fields **score** and **feedback** based on this answer where full marks is {marks} and can be given for the most precise and best answer, and 0 for wrong or irrelevant answers. Respond only with these two fields in JSON format.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    input_ids = tokenizer_qwen.encode(input_text, return_tensors=\"pt\").to(torch.device(\"cpu\"))\n",
        "\n",
        "    # Dynamically set the maximum number of new tokens\n",
        "    max_allowed_tokens = model_qwen.config.max_position_embeddings  # Total token limit of the model\n",
        "    input_token_count = len(input_ids[0])\n",
        "    max_new_tokens = max(50, max_allowed_tokens - input_token_count)  # Minimum 50 new tokens\n",
        "\n",
        "    # Generate the response\n",
        "    output_ids = model_qwen.generate(\n",
        "        input_ids,\n",
        "        max_length=input_token_count + max_new_tokens,  # Total length: input + generated tokens\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,  # Avoid repetition\n",
        "        early_stopping=True,    # Stop when the output is complete\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated output\n",
        "    return tokenizer_qwen.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "def extract_score_and_feedback(generated_text):\n",
        "    \"\"\"\n",
        "    Extract score and feedback from the generated text using regular expressions.\n",
        "\n",
        "    :param generated_text: The raw output from the model.\n",
        "    :return: A dictionary with \"score\" and \"feedback\".\n",
        "    \"\"\"\n",
        "    score_pattern = r'\"score\":\\s*([0-9.]+)'  # Matches numeric values for score\n",
        "    feedback_pattern = r'\"feedback\":\\s*\"([^\"]+)\"'  # Matches the feedback string\n",
        "\n",
        "    # Find matches\n",
        "    score_match = re.search(score_pattern, generated_text)\n",
        "    feedback_match = re.search(feedback_pattern, generated_text)\n",
        "\n",
        "    # Extract values\n",
        "    score = float(score_match.group(1)) if score_match else None  # Convert to float if match found\n",
        "    feedback = feedback_match.group(1) if feedback_match else None  # Extract feedback text if match found\n",
        "\n",
        "    return {\"score\": score, \"feedback\": feedback}\n",
        "\n",
        "\n",
        "def calculate_semantic_similarity(student_answer, expected_answer):\n",
        "    \"\"\"\n",
        "    Calculate the semantic similarity between the student's answer and the expected answer.\n",
        "\n",
        "    :param student_answer: The student's answer.\n",
        "    :param expected_answer: The expected correct answer.\n",
        "    :return: The semantic similarity score.\n",
        "    \"\"\"\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Encode both answers using the SentenceTransformer model\n",
        "    embeddings = sentence_transformer.encode([student_answer, expected_answer], convert_to_tensor=True)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity_score = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
        "    return similarity_score\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def GetData(question, student_answer, expected_answer, marks):\n",
        "    '''\n",
        "    Function to generate evaluation data based on the student's answer.\n",
        "\n",
        "    :param question: The question asked in the exam.\n",
        "    :param student_answer: The student's answer to the question.\n",
        "    :param expected_answer: The expected correct answer for the question.\n",
        "    :param marks: The full marks assigned to the question.\n",
        "\n",
        "    :return: A dictionary containing the analysis result.\n",
        "    '''\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Generate response using Qwen model\n",
        "    generated_text = generate_response(question, student_answer, marks)\n",
        "    # Extract score and feedback\n",
        "    result = extract_score_and_feedback(generated_text)\n",
        "    score = result['score']\n",
        "    feedback = result['feedback']\n",
        "\n",
        "    # Calculate semantic similarity between student's answer and expected answer\n",
        "    similarity = calculate_semantic_similarity(student_answer, expected_answer)\n",
        "\n",
        "    # Calculate final marks obtained based on score and semantic similarity\n",
        "    similarity_weight = 0.5  # You can adjust this weight based on your preference\n",
        "    final_marks_obtained = (score * 0.5) + (similarity * similarity_weight)\n",
        "\n",
        "    # Ensure that final_marks_obtained does not exceed the total marks\n",
        "    final_marks_obtained = min(final_marks_obtained, marks)\n",
        "\n",
        "    # Return the result as a dictionary\n",
        "    result_data = {\n",
        "        \"question\": question,\n",
        "        \"student_answer\": student_answer,\n",
        "        \"expected_answer\": expected_answer,\n",
        "        \"marks\": marks,\n",
        "        \"score\": score,\n",
        "        \"feedback\": feedback,\n",
        "        \"similarity\": similarity,\n",
        "        \"final_marks_obtained\": final_marks_obtained\n",
        "    }\n",
        "\n",
        "    return result_data\n",
        "\n",
        "\n",
        "\n",
        "    # Weighted sum of score and similarity\n",
        "    weighted_score = log_score * 0.5  # Adjust weight as needed\n",
        "    weighted_similarity = log_similarity * 0.5  # Adjust weight as needed\n",
        "\n",
        "    # Calculate an initial final marks based on the weighted sum\n",
        "    final_marks_obtained = weighted_score + weighted_similarity\n",
        "    final_marks_obtained = final_marks_obtained * marks  # Scale by full marks\n",
        "\n",
        "    # Apply penalty: if the final marks exceed 90% of full marks, reduce them\n",
        "    max_allowed_marks = marks * 0.9  # No student should get more than 90% of full marks\n",
        "    if final_marks_obtained > max_allowed_marks:\n",
        "        penalty_factor = 0.8  # Apply a 20% penalty\n",
        "        final_marks_obtained = final_marks_obtained * penalty_factor\n",
        "\n",
        "    # Ensure that the final marks are never below 0\n",
        "    final_marks_obtained = max(final_marks_obtained, 0)\n",
        "\n",
        "    final_marks_obtained = ( final_marks_obtained  + score) /2\n",
        "\n",
        "\n",
        "    # Calculate percentages\n",
        "    final_marks_percentage = round((final_marks_obtained / marks) * 100, 2) if marks > 0 else 0\n",
        "    score_percentage = round((score / marks) * 100, 2) if marks > 0 else 0\n",
        "\n",
        "    # Analyze the student's answer\n",
        "    student_word_count = len(student_answer.split())  # Count words in the student's answer\n",
        "    student_sentence_count = len(re.split(r'[.!?]+', student_answer.strip())) - 1  # Count sentences\n",
        "\n",
        "    # Return the result as a dictionary\n",
        "    result_data = {\n",
        "        \"question\": question,\n",
        "        \"student_answer\": student_answer,\n",
        "        \"expected_answer\": expected_answer,\n",
        "        \"marks\": marks,\n",
        "        \"score\": score,\n",
        "        \"score_percentage\": score_percentage,\n",
        "        \"feedback\": feedback,\n",
        "        \"similarity\": similarity,\n",
        "        \"similarity_percentage\": similarity_percentage,\n",
        "        \"final_marks_obtained\": final_marks_obtained,\n",
        "        \"final_marks_percentage\": final_marks_percentage,\n",
        "        \"student_word_count\": student_word_count,\n",
        "        \"student_sentence_count\": student_sentence_count\n",
        "    }\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "'''\n",
        "\n",
        "def GetData(question, student_answer, expected_answer, marks):\n",
        "    \"\"\"\n",
        "    Function to generate evaluation data based on the student's answer.\n",
        "\n",
        "    :param question: The question asked in the exam.\n",
        "    :param student_answer: The student's answer to the question.\n",
        "    :param expected_answer: The expected correct answer for the question.\n",
        "    :param marks: The full marks assigned to the question.\n",
        "\n",
        "    :return: A dictionary containing the analysis result.\n",
        "    \"\"\"\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Generate response using Qwen model\n",
        "    generated_text = generate_response(question, student_answer, marks)\n",
        "    # Extract score and feedback\n",
        "    result = extract_score_and_feedback(generated_text)\n",
        "    score = result['score']\n",
        "    feedback = result['feedback']\n",
        "\n",
        "    # Calculate semantic similarity between student's answer and expected answer\n",
        "    similarity = calculate_semantic_similarity(student_answer, expected_answer)\n",
        "    similarity_percentage = round(similarity * 100, 2)  # Convert similarity to percentage\n",
        "\n",
        "    # Logarithmic scaling of score and similarity\n",
        "    log_score = math.log(score + 1)  # Logarithm of score (score + 1 to avoid log(0))\n",
        "    log_similarity = math.log(similarity + 1)  # Logarithm of similarity (similarity + 1 to avoid log(0))\n",
        "\n",
        "    # Weighted sum of score and similarity\n",
        "    weighted_score = log_score * 0.5  # Adjust weight as needed\n",
        "    weighted_similarity = log_similarity * 0.5  # Adjust weight as needed\n",
        "\n",
        "    # Calculate an initial final marks based on the weighted sum\n",
        "    final_marks_obtained = weighted_score + weighted_similarity\n",
        "    final_marks_obtained = final_marks_obtained * marks  # Scale by full marks\n",
        "\n",
        "    # Apply penalty: if the final marks exceed 90% of full marks, reduce them\n",
        "    max_allowed_marks = marks * 0.9  # No student should get more than 90% of full marks\n",
        "    if final_marks_obtained > max_allowed_marks:\n",
        "        penalty_factor = 0.8  # Apply a 20% penalty\n",
        "        final_marks_obtained = final_marks_obtained * penalty_factor\n",
        "\n",
        "    # Ensure that the final marks are never below 0\n",
        "    final_marks_obtained = max(final_marks_obtained, 0)\n",
        "\n",
        "    final_marks_obtained = (final_marks_obtained + score) / 2\n",
        "\n",
        "    # Calculate percentages\n",
        "    final_marks_percentage = round((final_marks_obtained / marks) * 100, 2) if marks > 0 else 0\n",
        "    score_percentage = round((score / marks) * 100, 2) if marks > 0 else 0\n",
        "\n",
        "    # Analyze the student's answer\n",
        "    student_word_count = len(student_answer.split())  # Count words in the student's answer\n",
        "    student_sentence_count = len(re.split(r'[.!?]+', student_answer.strip())) - 1  # Count sentences\n",
        "\n",
        "    # Check for plagiarism\n",
        "    results_plagiarism = plagiarism_checker(student_answer)\n",
        "\n",
        "    # Detect AI-generated text\n",
        "    result_AI = detect_ai_generated_text_advanced_v3(student_answer, threshold=0.7)\n",
        "\n",
        "    # Return the result as a dictionary, including plagiarism and AI-detection data\n",
        "    result_data = {\n",
        "        \"question\": question,\n",
        "        \"student_answer\": student_answer,\n",
        "        \"expected_answer\": expected_answer,\n",
        "        \"marks\": marks,\n",
        "        \"score\": score,\n",
        "        \"score_percentage\": score_percentage,\n",
        "        \"feedback\": feedback,\n",
        "        \"similarity\": similarity,\n",
        "        \"similarity_percentage\": similarity_percentage,\n",
        "        \"final_marks_obtained\": final_marks_obtained,\n",
        "        \"final_marks_percentage\": final_marks_percentage,\n",
        "        \"student_word_count\": student_word_count,\n",
        "        \"student_sentence_count\": student_sentence_count,\n",
        "        \"plagiarism\": results_plagiarism[\"plagiarism\"],\n",
        "        \"average_score\": results_plagiarism[\"average_score\"],\n",
        "        \"max_score\": results_plagiarism[\"max_score\"],\n",
        "        \"max_score_url\": results_plagiarism[\"max_score_url\"],\n",
        "        \"plagiarism_data\": results_plagiarism[\"data\"],\n",
        "        \"AI_category\": result_AI[\"category\"],\n",
        "        \"AI_flag\": result_AI[\"AI\"],\n",
        "        \"AI_percentage\": result_AI[\"AI_Percentage\"],\n",
        "        \"AI_confidence\": result_AI[\"confidence\"],\n",
        "        \"AI_confidence_difference\": result_AI[\"confidence_difference\"],\n",
        "        \"AI_probabilities\": result_AI[\"probabilities\"],\n",
        "        \"AI_metrics\": result_AI[\"metrics\"],\n",
        "        \"AI_threshold_used\": result_AI[\"threshold_used\"],\n",
        "    }\n",
        "\n",
        "    return result_data\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "import math\n",
        "import re\n",
        "\n",
        "def GetData(question, student_answer, expected_answer, marks):\n",
        "    \"\"\"\n",
        "    Function to generate evaluation data based on the student's answer.\n",
        "\n",
        "    :param question: The question asked in the exam.\n",
        "    :param student_answer: The student's answer to the question.\n",
        "    :param expected_answer: The expected correct answer for the question.\n",
        "    :param marks: The full marks assigned to the question.\n",
        "\n",
        "    :return: A dictionary containing the analysis result.\n",
        "    \"\"\"\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Ensure non-empty inputs for student_answer and question\n",
        "    student_answer = student_answer or \"\"  # Default to empty string if None or empty\n",
        "    expected_answer = expected_answer or \"\"  # Default to empty string if None or empty\n",
        "    marks = marks or 0  # Default to 0 if marks are None or empty\n",
        "    question = question or \"\"  # Default to empty string if None or empty\n",
        "\n",
        "    # Generate response using Qwen model\n",
        "    generated_text = generate_response(question, student_answer, marks)\n",
        "\n",
        "    # Extract score and feedback\n",
        "    result = extract_score_and_feedback(generated_text)\n",
        "    score = result.get('score', 0)  # Default to 0 if score is None or not found\n",
        "    feedback = result.get('feedback', \"\")  # Default to empty string if feedback is None or not found\n",
        "\n",
        "    # Calculate semantic similarity between student's answer and expected answer\n",
        "    similarity = calculate_semantic_similarity(student_answer, expected_answer) or 0  # Default to 0 if None\n",
        "    similarity_percentage = round(similarity * 100, 2)  # Convert similarity to percentage\n",
        "\n",
        "    # Logarithmic scaling of score and similarity\n",
        "    log_score = math.log(score + 1) if score is not None else 0  # Logarithm of score (score + 1 to avoid log(0))\n",
        "    log_similarity = math.log(similarity + 1) if similarity is not None else 0  # Logarithm of similarity (similarity + 1 to avoid log(0))\n",
        "\n",
        "    # Weighted sum of score and similarity\n",
        "    weighted_score = log_score * 0.5  # Adjust weight as needed\n",
        "    weighted_similarity = log_similarity * 0.5  # Adjust weight as needed\n",
        "\n",
        "    # Calculate an initial final marks based on the weighted sum\n",
        "    final_marks_obtained = weighted_score + weighted_similarity\n",
        "    final_marks_obtained = final_marks_obtained * marks  # Scale by full marks\n",
        "\n",
        "    # Apply penalty: if the final marks exceed 90% of full marks, reduce them\n",
        "    max_allowed_marks = marks * 0.9  # No student should get more than 90% of full marks\n",
        "    if final_marks_obtained > max_allowed_marks:\n",
        "        penalty_factor = 0.8  # Apply a 20% penalty\n",
        "        final_marks_obtained = final_marks_obtained * penalty_factor\n",
        "\n",
        "    # Ensure that the final marks are never below 0\n",
        "    final_marks_obtained = max(final_marks_obtained, 0)\n",
        "\n",
        "    final_marks_obtained = (final_marks_obtained + score) / 2\n",
        "\n",
        "    # Calculate percentages\n",
        "    final_marks_percentage = round((final_marks_obtained / marks) * 100, 2) if marks > 0 else 0\n",
        "    score_percentage = round((score / marks) * 100, 2) if marks > 0 else 0\n",
        "\n",
        "    # Analyze the student's answer\n",
        "    student_word_count = len(student_answer.split())  # Count words in the student's answer\n",
        "    student_sentence_count = len(re.split(r'[.!?]+', student_answer.strip())) - 1  # Count sentences\n",
        "\n",
        "    # Check for plagiarism\n",
        "    results_plagiarism = plagiarism_checker(student_answer) or {}\n",
        "\n",
        "    # Detect AI-generated text\n",
        "    result_AI = detect_ai_generated_text_advanced_v3(student_answer, threshold=0.7) or {}\n",
        "\n",
        "    # Return the result as a dictionary, including plagiarism and AI-detection data\n",
        "    result_data = {\n",
        "        \"question\": question,\n",
        "        \"student_answer\": student_answer,\n",
        "        \"expected_answer\": expected_answer,\n",
        "        \"marks\": marks,\n",
        "        \"score\": score,\n",
        "        \"score_percentage\": score_percentage,\n",
        "        \"feedback\": feedback,\n",
        "        \"similarity\": similarity,\n",
        "        \"similarity_percentage\": similarity_percentage,\n",
        "        \"final_marks_obtained\": final_marks_obtained,\n",
        "        \"final_marks_percentage\": final_marks_percentage,\n",
        "        \"student_word_count\": student_word_count,\n",
        "        \"student_sentence_count\": student_sentence_count,\n",
        "        \"plagiarism\": results_plagiarism.get(\"plagiarism\", False),\n",
        "        \"average_score\": results_plagiarism.get(\"average_score\", 0.0),\n",
        "        \"max_score\": results_plagiarism.get(\"max_score\", 0.0),\n",
        "        \"max_score_url\": results_plagiarism.get(\"max_score_url\", \"\"),\n",
        "        \"plagiarism_data\": results_plagiarism.get(\"data\", \"\"),\n",
        "        \"AI_category\": result_AI.get(\"category\", \"Unknown\"),\n",
        "        \"AI_flag\": result_AI.get(\"AI\", False),\n",
        "        \"AI_percentage\": result_AI.get(\"AI_Percentage\", 0.0),\n",
        "        \"AI_confidence\": result_AI.get(\"confidence\", 0.0),\n",
        "        \"AI_confidence_difference\": result_AI.get(\"confidence_difference\", 0.0),\n",
        "        \"AI_probabilities\": result_AI.get(\"probabilities\", {}),\n",
        "        \"AI_metrics\": result_AI.get(\"metrics\", {}),\n",
        "        \"AI_threshold_used\": result_AI.get(\"threshold_used\", 0.0),\n",
        "    }\n",
        "\n",
        "    return result_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import hashlib\n",
        "import json\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Ensure NLTK resources are downloaded for sentence tokenization\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    print(\"Downloading 'punkt' package...\")\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "def get_useragent():\n",
        "    _useragent_list = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.62',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0'\n",
        "    ]\n",
        "    return random.choice(_useragent_list)\n",
        "\n",
        "def google_search(query):\n",
        "    links = []\n",
        "    try:\n",
        "        headers = {\"User-Agent\": get_useragent()}\n",
        "        params = {\"q\": f'\"{query}\"', \"num\": 3, \"hl\": 'en'}\n",
        "\n",
        "        # Perform the Google search request\n",
        "        response = requests.get(\"https://www.google.com/search\", headers=headers, params=params, timeout=5)\n",
        "        response.raise_for_status()  # Raises an error for bad responses\n",
        "\n",
        "        # Parse the response content with BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract URLs from the search result blocks (avoiding ads, etc.)\n",
        "\n",
        "        try:\n",
        "            for result in soup.find_all('h3'):\n",
        "                link = result.find_parent('a')\n",
        "                if link:\n",
        "                    link_text = link.get('href')\n",
        "                    links.append(link_text)\n",
        "        except:\n",
        "            for link_tag in soup.select('div.yuRUbf a'):\n",
        "                link = link_tag.get('href')\n",
        "                if link:\n",
        "                    links.append(link)\n",
        "\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error during the search request: {e}\")\n",
        "\n",
        "    return links\n",
        "\n",
        "def fetch_web_content(url: str) -> str:\n",
        "    \"\"\"Fetch and clean the main content of a webpage.\"\"\"\n",
        "    try:\n",
        "        headers = {\"User-Agent\": get_useragent()}\n",
        "        response = requests.get(url, headers=headers, timeout=5)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        return \" \".join([para.get_text() for para in paragraphs])\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to fetch {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_shingles(text: str, k: int = 5) -> set:\n",
        "    \"\"\"Generate k-shingles (sets of k consecutive words) for a given text.\"\"\"\n",
        "    words = text.split()\n",
        "    shingles = set()\n",
        "    for i in range(len(words) - k + 1):\n",
        "        shingle = \" \".join(words[i:i + k])\n",
        "        shingle_hash = hashlib.md5(shingle.encode(\"utf-8\")).hexdigest()\n",
        "        shingles.add(shingle_hash)\n",
        "    return shingles\n",
        "\n",
        "def similarity1(set1, set2):\n",
        "    # Check if either set is empty to prevent division by zero\n",
        "    if len(set1) == 0 or len(set2) == 0:\n",
        "        return 0.0\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    return (intersection / len(set1)) * 100\n",
        "\n",
        "def calculate_jaccard_similarity(shingles1: set, shingles2: set) -> float:\n",
        "    \"\"\"Calculate Jaccard similarity between two sets of shingles.\"\"\"\n",
        "    intersection = shingles1.intersection(shingles2)\n",
        "    union = shingles1.union(shingles2)\n",
        "\n",
        "    # Avoid division by zero if both sets are empty\n",
        "    if len(union) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return len(intersection) / len(union)\n",
        "\n",
        "def check_sentence_plagiarism(sentence: str) -> Dict[str, Any]:\n",
        "    \"\"\"Check plagiarism for a single sentence by searching and comparing content.\"\"\"\n",
        "    result = {\"sentence\": sentence, \"matches\": []}\n",
        "    urls = google_search(sentence)\n",
        "\n",
        "    for url in urls:\n",
        "        content = fetch_web_content(url)\n",
        "        if content:\n",
        "            original_shingles = get_shingles(sentence)\n",
        "            content_shingles = get_shingles(content)\n",
        "            # similarity = calculate_jaccard_similarity(original_shingles, content_shingles)\n",
        "            similarity = similarity1(original_shingles, content_shingles)\n",
        "            result[\"matches\"].append({\"url\": url, \"score\": similarity})\n",
        "\n",
        "    # Sort matches by score in descending order and take the highest\n",
        "    if result[\"matches\"]:\n",
        "        result[\"matches\"].sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "        highest_match = result[\"matches\"][0]\n",
        "        result[\"highest_match\"] = {\"url\": highest_match[\"url\"], \"score\": highest_match[\"score\"]}\n",
        "    else:\n",
        "        result[\"highest_match\"] = {\"url\": None, \"score\": 0.0}\n",
        "\n",
        "    return result\n",
        "\n",
        "def plagiarism_checker(text: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Run plagiarism check on each sentence in the input text.\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    results = [check_sentence_plagiarism(sentence) for sentence in sentences]\n",
        "    return get_score(results)\n",
        "\n",
        "def get_score(data):\n",
        "    # Merging sentences by URL and score\n",
        "    result = []\n",
        "    total_score = 0\n",
        "    max_score = 0\n",
        "    max_score_url = \"\"\n",
        "    total_sentences = len(data)\n",
        "\n",
        "    for entry in data:\n",
        "        url = entry['highest_match']['url']\n",
        "        score = entry['highest_match']['score']\n",
        "        sentence = entry['sentence']\n",
        "\n",
        "        # Check if score is greater than 40 for plagiarism flag\n",
        "        plagiarism = True if score > 40 else False\n",
        "\n",
        "        # Add score to total score\n",
        "        total_score += score\n",
        "\n",
        "        # Track maximum score and corresponding URL\n",
        "        if score > max_score:\n",
        "            max_score = score\n",
        "            max_score_url = url\n",
        "\n",
        "        # Check if we should merge with the last entry\n",
        "        if result and result[-1]['url'] == url and result[-1]['score'] == score:\n",
        "            result[-1]['sentence'] += \" \" + sentence  # Append the sentence\n",
        "        else:\n",
        "            # Add a new entry with plagiarism flag\n",
        "            result.append({\n",
        "                'sentence': sentence,\n",
        "                'url': url,\n",
        "                'score': score,\n",
        "                'plagiarism': plagiarism\n",
        "            })\n",
        "\n",
        "    # Calculate average score\n",
        "    average_score = total_score / total_sentences if total_sentences else 0\n",
        "\n",
        "    # Create the final structured response\n",
        "    response = {\n",
        "        \"plagiarism\": any(entry['plagiarism'] for entry in result),\n",
        "        \"average_score\": average_score,\n",
        "        \"max_score\": max_score,\n",
        "        \"max_score_url\": max_score_url,\n",
        "        \"data\": result\n",
        "    }\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "# input_text = \"\"\"\n",
        "# An Operating System can be defined as an interface between user and hardware. It is responsible for the execution of all the processes, Resource Allocation, CPU ...\"\"\"\n",
        "# results_plagiarism= plagiarism_checker(input_text)\n",
        "# print(json.dumps(results_plagiarism, indent=2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def detect_ai_generated_text_advanced_v3(\n",
        "    text,\n",
        "    model_name=\"roberta-base-openai-detector\",\n",
        "    threshold=0.5,\n",
        "    max_length=512\n",
        "):\n",
        "    \"\"\"\n",
        "    Advanced detection of AI-generated text with detailed metrics, AI flag, and percentage.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): Input text to analyze.\n",
        "        model_name (str): Pre-trained model to use for detection.\n",
        "        threshold (float): Confidence threshold for categorization (default: 0.5).\n",
        "        max_length (int): Maximum length for tokenization (default: 512).\n",
        "\n",
        "    Returns:\n",
        "        dict: Detailed output with categorization, confidence, metrics, AI flag, and percentage.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load tokenizer and model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "        # Tokenize and prepare text\n",
        "        tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "        token_count = len(tokens[\"input_ids\"][0])\n",
        "\n",
        "        # Inference\n",
        "        outputs = model(**tokens)\n",
        "        probabilities = torch.softmax(outputs.logits, dim=1).detach().numpy()[0]\n",
        "        ai_confidence = probabilities[1]\n",
        "        human_confidence = probabilities[0]\n",
        "        category = \"AI-generated\" if ai_confidence > threshold else \"Human-written\"\n",
        "\n",
        "        # AI flag and percentage\n",
        "        is_ai = ai_confidence > threshold\n",
        "        ai_percentage = round(ai_confidence * 100, 2)\n",
        "\n",
        "        # Additional metrics\n",
        "        char_count = len(text)\n",
        "        word_count = len(text.split())\n",
        "        avg_word_length = char_count / word_count if word_count > 0 else 0\n",
        "        confidence_diff = abs(ai_confidence - human_confidence)\n",
        "\n",
        "        return {\n",
        "            \"category\": category,\n",
        "            \"AI\": is_ai,\n",
        "            \"AI_Percentage\": ai_percentage,\n",
        "            \"confidence\": ai_confidence if is_ai else human_confidence,\n",
        "            \"confidence_difference\": confidence_diff,\n",
        "            \"probabilities\": {\n",
        "                \"Human-written\": human_confidence,\n",
        "                \"AI-generated\": ai_confidence\n",
        "            },\n",
        "            \"metrics\": {\n",
        "                \"character_count\": char_count,\n",
        "                \"word_count\": word_count,\n",
        "                \"average_word_length\": round(avg_word_length, 2),\n",
        "                \"token_count\": token_count,\n",
        "                \"max_token_length\": max_length\n",
        "            },\n",
        "            \"threshold_used\": threshold\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# Example usage\n",
        "#text = \"In a world driven by rapid technological advancement, artificial intelligence has emerged as a transformative force. From revolutionizing healthcare with predictive diagnostics to enhancing efficiency in industries through automation, AI continues to redefine the boundaries of what is possible. However, this unprecedented growth also poses ethical dilemmas, emphasizing the need for responsible innovation to ensure these technologies benefit humanity as a whole.\"\n",
        "#result_AI = detect_ai_generated_text_advanced_v3(text, threshold=0.7)\n",
        "#print(result)\n",
        "#print()\n",
        "\n",
        "def convert_numpy_types(obj):\n",
        "    if isinstance(obj, np.generic):  # Check if itâ€™s a numpy scalar\n",
        "        return obj.item()  # Convert to native Python type\n",
        "    return obj  # Leave other types unchanged\n",
        "\n",
        "# load models for first time\n",
        "load_models()  # Load models once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCqadeHg9vhF",
        "outputId": "d2e4ec95-c847-4d24-b41c-6ae35793c4f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"question\": \"What is the purpose of a file system?\",\n",
            "    \"student_answer\": \"An operating system (OS) is system software that manages computer hardware and software resources, and provides common services for computer programs.\",\n",
            "    \"expected_answer\": \"A file system organizes and controls how data is stored and retrieved on a computer system.\",\n",
            "    \"marks\": 5,\n",
            "    \"score\": 1.0,\n",
            "    \"score_percentage\": 20.0,\n",
            "    \"feedback\": \"The provided answer does not directly address the question about the primary purpose(s) of an operating systems (operating system). The answer focuses more on explaining what an OS is rather than its main function. It correctly identifies the definition but misses out on the central concept asked for.\",\n",
            "    \"similarity\": 0.4761287569999695,\n",
            "    \"similarity_percentage\": 47.61,\n",
            "    \"final_marks_obtained\": 1.8532126708417516,\n",
            "    \"final_marks_percentage\": 37.06,\n",
            "    \"student_word_count\": 21,\n",
            "    \"student_sentence_count\": 1,\n",
            "    \"plagiarism\": false,\n",
            "    \"average_score\": 0.0,\n",
            "    \"max_score\": 0,\n",
            "    \"max_score_url\": \"\",\n",
            "    \"plagiarism_data\": [\n",
            "        {\n",
            "            \"sentence\": \"An operating system (OS) is system software that manages computer hardware and software resources, and provides common services for computer programs.\",\n",
            "            \"url\": null,\n",
            "            \"score\": 0.0,\n",
            "            \"plagiarism\": false\n",
            "        }\n",
            "    ],\n",
            "    \"AI_category\": \"Human-written\",\n",
            "    \"AI_flag\": false,\n",
            "    \"AI_percentage\": 41.15,\n",
            "    \"AI_confidence\": 0.5884890556335449,\n",
            "    \"AI_confidence_difference\": 0.17697817087173462,\n",
            "    \"AI_probabilities\": {\n",
            "        \"Human-written\": 0.5884890556335449,\n",
            "        \"AI-generated\": 0.4115108847618103\n",
            "    },\n",
            "    \"AI_metrics\": {\n",
            "        \"character_count\": 150,\n",
            "        \"word_count\": 21,\n",
            "        \"average_word_length\": 7.14,\n",
            "        \"token_count\": 27,\n",
            "        \"max_token_length\": 512\n",
            "    },\n",
            "    \"AI_threshold_used\": 0.7\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import json\n",
        "question = \"What is the purpose of a file system?\"\n",
        "student_answer = \"An operating system (OS) is system software that manages computer hardware and software resources, and provides common services for computer programs.\"\n",
        "expected_answer = \"A file system organizes and controls how data is stored and retrieved on a computer system.\"\n",
        "marks = 5  # Full marks for the question\n",
        "\n",
        "# Get the evaluation data\n",
        "evaluation_result = GetData(question, student_answer, expected_answer, marks)\n",
        "\n",
        "result_json = json.dumps(evaluation_result, indent=4, default=convert_numpy_types)\n",
        "print(result_json)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5qj8CYHDpsEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bellow is the working code"
      ],
      "metadata": {
        "id": "8eigC2HbnA6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import asyncio\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = '/content/drive/MyDrive/sses/cleaned_lsngjlxe_sses.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Define the starting row\n",
        "start_row = 80  # 81st row in Excel (0-based indexing)\n",
        "\n",
        "# Define the output file path\n",
        "output_file_path = '/content/drive/MyDrive/sses/output_results.xlsx'\n",
        "\n",
        "# Load existing results if available\n",
        "try:\n",
        "    existing_df = pd.read_excel(output_file_path)\n",
        "    print(\"Existing results loaded.\")\n",
        "except FileNotFoundError:\n",
        "    # Initialize an empty DataFrame with the necessary columns\n",
        "    existing_df = pd.DataFrame(columns=df.columns.tolist() + ['evaluation_result_json'])\n",
        "    print(\"Output file not found. Starting fresh.\")\n",
        "\n",
        "# Async function to save DataFrame\n",
        "async def save_to_drive(df, file_path):\n",
        "    await asyncio.to_thread(df.to_excel, file_path, index=False)\n",
        "    print(f\"Data saved to {file_path}.\")\n",
        "\n",
        "# Main processing function\n",
        "async def process_rows():\n",
        "    global existing_df  # Ensure we modify the global variable\n",
        "    for index, row in df.iloc[start_row:].iterrows():\n",
        "        question = row['question_text']\n",
        "        student_answer = row['student_answer']\n",
        "        full_marks = row['full_marks']\n",
        "        expected_answer = \"\"  # As expected_answer is not present, send an empty value\n",
        "\n",
        "        try:\n",
        "            # Call the GetData function\n",
        "            evaluation_result = GetData(question, student_answer, expected_answer, full_marks)\n",
        "            result_json = json.dumps(evaluation_result, indent=4, default=convert_numpy_types)\n",
        "\n",
        "            # Add the evaluation result to the current row\n",
        "            row_data = row.to_dict()\n",
        "            row_data['evaluation_result_json'] = result_json\n",
        "\n",
        "            # Convert the row to a DataFrame and append to the existing DataFrame\n",
        "            new_row_df = pd.DataFrame([row_data])\n",
        "            existing_df = pd.concat([existing_df, new_row_df], ignore_index=True)\n",
        "\n",
        "            # Save to Google Drive asynchronously\n",
        "            await save_to_drive(existing_df, output_file_path)\n",
        "\n",
        "            # Print success message and JSON response\n",
        "            print(f\"Row {index + 1} processed and saved:\\n{result_json}\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing row {index + 1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(\"All rows processed and saved.\")\n",
        "\n",
        "# Run the async processing function\n",
        "await process_rows()"
      ],
      "metadata": {
        "id": "aU9mX5UVnSRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is temp"
      ],
      "metadata": {
        "id": "vO5k3WNdnEo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import asyncio\n",
        "from time import time\n",
        "\n",
        "# File paths\n",
        "input_file_path = '/content/drive/MyDrive/sses/cleaned_lsngjlxe_sses.xlsx'\n",
        "output_file_path = '/content/drive/MyDrive/sses/output_results.xlsx'\n",
        "\n",
        "# Parameters\n",
        "start_row = 80  # Start processing from row 81 (0-based index)\n",
        "retry_attempts = 3  # Number of retries for GetData\n",
        "save_interval = 1  # Save after each row\n",
        "\n",
        "# Load the input file\n",
        "df = pd.read_excel(input_file_path)\n",
        "\n",
        "# Load existing results if available\n",
        "try:\n",
        "    existing_df = pd.read_excel(output_file_path)\n",
        "    print(\"Existing results loaded.\")\n",
        "    last_saved_row = len(existing_df)\n",
        "except FileNotFoundError:\n",
        "    existing_df = pd.DataFrame(columns=df.columns.tolist() + ['evaluation_result_json'])\n",
        "    print(\"Output file not found. Starting fresh.\")\n",
        "    last_saved_row = 0\n",
        "\n",
        "# Initialize buffer for new rows\n",
        "new_rows = []\n",
        "\n",
        "# Async function to save progress\n",
        "async def save_progress():\n",
        "    global last_saved_row\n",
        "    if new_rows:\n",
        "        df_to_save = pd.DataFrame(new_rows)\n",
        "        df_to_save.to_excel(output_file_path, mode='a', header=False, index=False)\n",
        "        new_rows.clear()\n",
        "        last_saved_row += len(df_to_save)\n",
        "        print(f\"Last saved row in drive: {last_saved_row}\")\n",
        "\n",
        "# Main processing function\n",
        "async def process_rows():\n",
        "    global existing_df, new_rows\n",
        "\n",
        "    for index, row in df.iloc[start_row:].iterrows():\n",
        "        start_time = time()\n",
        "        print(f\"Currently processing row: {index + 1}\")\n",
        "        question = row['question_text']\n",
        "        student_answer = row['student_answer']\n",
        "        full_marks = row['full_marks']\n",
        "        expected_answer = \"\"  # No expected answer\n",
        "\n",
        "        # Retry mechanism for GetData\n",
        "        for attempt in range(retry_attempts):\n",
        "            try:\n",
        "                # Call GetData\n",
        "                evaluation_result = GetData(question, student_answer, expected_answer, full_marks)\n",
        "\n",
        "                # Add the evaluation result to the row\n",
        "                row_data = row.to_dict()\n",
        "                row_data['evaluation_result_json'] = json.dumps(evaluation_result, default=str)\n",
        "                new_rows.append(row_data)\n",
        "\n",
        "                # Save progress after each row\n",
        "                if len(new_rows) >= save_interval:\n",
        "                    await asyncio.to_thread(save_progress)\n",
        "\n",
        "                print(f\"Row {index + 1} processed in {time() - start_time:.2f} seconds.\")\n",
        "                break  # Exit retry loop on success\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing row {index + 1}, attempt {attempt + 1}: {e}\")\n",
        "                if attempt == retry_attempts - 1:\n",
        "                    print(f\"Skipping row {index + 1} after {retry_attempts} attempts.\")\n",
        "                else:\n",
        "                    await asyncio.sleep(0.5)  # Short delay before retrying\n",
        "                continue\n",
        "\n",
        "    # Final save for remaining rows\n",
        "    await asyncio.to_thread(save_progress)\n",
        "    print(\"All rows processed and saved.\")\n",
        "\n",
        "# Run the async processing function\n",
        "await process_rows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646,
          "referenced_widgets": [
            "4054a3f43a254da889c0f385a726f039",
            "98b4b4e6953b451b9f54a664f540f5fe",
            "9b214ea429f149dfbb9d835a5aa15c35",
            "b1fb64a1db84488595a85c7aa1d914fd",
            "97fd35b186474fde9e8c21ca6a59c744",
            "e3ab126bc08a44828cb8c72fd4608737",
            "30642a30289445c384fdfc62f29cd92d",
            "ab449d4e17974ec081d0c955a19e51e9",
            "1e39f17c86424fafba1defe4ac585f5a",
            "8a3740f5eb6f4099a9b09984ee4d5cb1",
            "1f8a7ddb80154d07904560c9317785b4",
            "7274285987d74e31a7dcc1adf86daa9e",
            "ff5a0530b13e4dac8280c86293a8190d",
            "de2637e3dc7d462386f79512944a7032",
            "c7ae3710c8074a00943c445b4de8d5a1",
            "368706dd3df2482181c17ee3b3835192",
            "cf85a8ce87f04ee8acf84e171352de84",
            "65276d7e6d5e4de7b454794156285663",
            "1f55026b61a44272994c418a6a59db66",
            "a10c0168f4fd4034a47a5728fef4a9ee",
            "7f1aa59fd7ea42f4ac246f23c9109a2c",
            "6257942ecca6403e82d87e1ed913e5a8",
            "751ba946e23f421493d54f66cd474302",
            "27b580787091416c8c3e4b9fccfffa05",
            "517909cf302b4475a51a291c13865eac",
            "5178126832d94b1e9bdbc9bb963185ef",
            "8a1a0d957a6f467e949e8f87a3d7267e",
            "57f48e86cece4252b9fa3331336d17b2",
            "a303b26e83544b05a463a4f2d2991bee",
            "31926ab9781747039443c4cf684f6492",
            "091d07595c404d2ea29d0fae59544d02",
            "58bb11cbf3ea4e46a5010d53cff62e7e",
            "f1d3294d6d6e412ebd108320be699942",
            "d8448f12e45045f183fed5cafc242d0c",
            "9c1de39b68a74b8c826f7577f5807685",
            "35f2e6850156425fbf20909ae55c0bf8",
            "41f9a65643c844df963104cda591fca3",
            "7d488776125347a9a22a29b552f8d2d8",
            "456e196b33e34842b36a853a8283a528",
            "ce26b9bdbd7042368c705c9b68926fa2",
            "e32f86db94e2413c8d3449881f47413a",
            "fe8dc9b2440d43bf8f9f06c06fe7e64a",
            "5b52df9d70f243e4bdb23b421afcea7f",
            "df0aa19fa9d54acb9e253ef5ed8fb1fa",
            "92aec70dfee0486ea27ce00503432e3e",
            "cd1e453bbf4943a98fec5d5063a521e7",
            "699569dcca0a4f3992d106696877f837",
            "0492d2091adc4e4db367d58deb2e8ea0",
            "ee20871b358e4b9086f575555591dff2",
            "cbc4d5b75bd0460faaf146658bbacb9e",
            "537087f9e22648209ca533ad53dec3f7",
            "9278d2848d2145d8bd6e6af81fd1c476",
            "92821faaf6834591bfa619a90f14643f",
            "5d318fbff86a414999bf5f557fce6291",
            "4f4f9fd677d44921b284b9f829941ecc",
            "31b7a3c43d354884a9e211de6520f549",
            "bfb22529bf1b4b12853ad54542daf703",
            "de81bff4716146efb56168d48d37eccb",
            "d6cb201b1e59470b98fcf02e0d741a37",
            "69dafaac7a9a4ba49405d3554efb8f65",
            "7636a86187ba417b99194487b363ea17",
            "76c70e48f2a84fc9b7f5af62e26fb804",
            "7a131639a4284f6391b793ea1c39c694",
            "730397abcd82405f954dd926551b3ba6",
            "5a6edb02187e46069afe8f38edd6948b",
            "6eefc4268ab844b4b4186a878756d916"
          ]
        },
        "id": "SV_iNKEXoz9D",
        "outputId": "ac9fa7ec-7279-4e9b-8afb-6be7ba1feeb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing results loaded.\n",
            "Currently processing row: 81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4054a3f43a254da889c0f385a726f039"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7274285987d74e31a7dcc1adf86daa9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "751ba946e23f421493d54f66cd474302"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8448f12e45045f183fed5cafc242d0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92aec70dfee0486ea27ce00503432e3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31b7a3c43d354884a9e211de6520f549"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 81 processed in 50.17 seconds.\n",
            "Currently processing row: 82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/lib/python3.11/asyncio/base_events.py:1921: RuntimeWarning: coroutine 'save_progress' was never awaited\n",
            "  handle = self._ready.popleft()\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 82 processed in 38.62 seconds.\n",
            "Currently processing row: 83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2nd temp"
      ],
      "metadata": {
        "id": "EKGipeN5qbUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import asyncio\n",
        "from time import time\n",
        "import numpy as np\n",
        "\n",
        "# File paths\n",
        "input_file_path = '/content/drive/MyDrive/sses/cleaned_lsngjlxe_sses.xlsx'\n",
        "output_file_path = '/content/drive/MyDrive/sses/output_results.xlsx'\n",
        "\n",
        "# Parameters\n",
        "start_row = 80  # Start processing from row 81 (0-based index)\n",
        "retry_attempts = 3  # Number of retries for GetData\n",
        "save_interval = 1  # Save after each row\n",
        "\n",
        "# Load the input file\n",
        "df = pd.read_excel(input_file_path)\n",
        "\n",
        "# Load existing results if available\n",
        "try:\n",
        "    existing_df = pd.read_excel(output_file_path)\n",
        "    print(\"Existing results loaded.\")\n",
        "    last_saved_row = len(existing_df)\n",
        "except FileNotFoundError:\n",
        "    existing_df = pd.DataFrame(columns=df.columns.tolist() + ['evaluation_result_json'])\n",
        "    print(\"Output file not found. Starting fresh.\")\n",
        "    last_saved_row = 0\n",
        "\n",
        "# Initialize buffer for new rows\n",
        "new_rows = []\n",
        "\n",
        "# Function to handle NumPy data types in JSON\n",
        "def convert_numpy_types(obj):\n",
        "    if isinstance(obj, (np.integer, int)):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, (np.floating, float)):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, (np.bool_, bool)):\n",
        "        return bool(obj)\n",
        "    elif obj is None:\n",
        "        return None\n",
        "    raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
        "\n",
        "# Async function to save progress\n",
        "async def save_progress():\n",
        "    global last_saved_row\n",
        "    if new_rows:\n",
        "        df_to_save = pd.DataFrame(new_rows)\n",
        "        df_to_save.to_excel(output_file_path, mode='a', header=False, index=False)\n",
        "        new_rows.clear()\n",
        "        last_saved_row += len(df_to_save)\n",
        "        print(f\"Last saved row in drive: {last_saved_row}\")\n",
        "\n",
        "# Main processing function\n",
        "async def process_rows():\n",
        "    global existing_df, new_rows\n",
        "\n",
        "    for index, row in df.iloc[start_row:].iterrows():\n",
        "        start_time = time()\n",
        "        print(f\"\\nCurrently processing row: {index + 1}\")\n",
        "        question = row['question_text']\n",
        "        student_answer = row['student_answer']\n",
        "        full_marks = row['full_marks']\n",
        "        expected_answer = \"\"  # No expected answer\n",
        "\n",
        "        # Retry mechanism for GetData\n",
        "        for attempt in range(retry_attempts):\n",
        "            try:\n",
        "                # Call GetData\n",
        "                evaluation_result = GetData(question, student_answer, expected_answer, full_marks)\n",
        "\n",
        "                # Add the evaluation result to the row\n",
        "                row_data = row.to_dict()\n",
        "                row_data['evaluation_result_json'] = json.dumps(evaluation_result, default=convert_numpy_types)\n",
        "                new_rows.append(row_data)\n",
        "\n",
        "                # Display processing result\n",
        "                print(f\"Processing result (JSON): {json.dumps(evaluation_result, indent=4, default=convert_numpy_types)}\")\n",
        "\n",
        "                # Save progress after each row\n",
        "                if len(new_rows) >= save_interval:\n",
        "                    await asyncio.to_thread(save_progress)\n",
        "\n",
        "                print(f\"Row {index + 1} processed in {time() - start_time:.2f} seconds.\")\n",
        "                break  # Exit retry loop on success\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing row {index + 1}, attempt {attempt + 1}: {e}\")\n",
        "                if attempt == retry_attempts - 1:\n",
        "                    print(f\"Skipping row {index + 1} after {retry_attempts} attempts.\")\n",
        "                else:\n",
        "                    await asyncio.sleep(0.5)  # Short delay before retrying\n",
        "                continue\n",
        "\n",
        "    # Final save for remaining rows\n",
        "    await asyncio.to_thread(save_progress)\n",
        "    print(\"All rows processed and saved.\")\n",
        "\n",
        "# Run the async processing function\n",
        "await process_rows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7xJhSIdqcgS",
        "outputId": "b10f0954-2ce5-4324-89b9-d75446d83954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing results loaded.\n",
            "\n",
            "Currently processing row: 81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing result (JSON): {\n",
            "    \"question\": \"Q7. Caesar Cipher Decryption: Decrypt the cipher text \\\"LQIRUPDWLRQ VHFULWB\\\", which was encrypted using a Caesar Cipher with a shift of 3. (2 Marks)_x000D_\\n\",\n",
            "    \"student_answer\": \"\\nInformation security\",\n",
            "    \"expected_answer\": \"\",\n",
            "    \"marks\": 2.0,\n",
            "    \"score\": 1.5,\n",
            "    \"score_percentage\": 75.0,\n",
            "    \"feedback\": \"The answer correctly decrypts the Caesar cipher but could use more details about the encryption process and steps taken to arrive at the solution.\",\n",
            "    \"similarity\": 0.12042701244354248,\n",
            "    \"similarity_percentage\": 12.04,\n",
            "    \"final_marks_obtained\": 1.2650003028149577,\n",
            "    \"final_marks_percentage\": 63.25,\n",
            "    \"student_word_count\": 2,\n",
            "    \"student_sentence_count\": 0,\n",
            "    \"plagiarism\": false,\n",
            "    \"average_score\": 0.0,\n",
            "    \"max_score\": 0,\n",
            "    \"max_score_url\": \"\",\n",
            "    \"plagiarism_data\": [\n",
            "        {\n",
            "            \"sentence\": \"\\nInformation security\",\n",
            "            \"url\": null,\n",
            "            \"score\": 0.0,\n",
            "            \"plagiarism\": false\n",
            "        }\n",
            "    ],\n",
            "    \"AI_category\": \"Human-written\",\n",
            "    \"AI_flag\": false,\n",
            "    \"AI_percentage\": 15.1,\n",
            "    \"AI_confidence\": 0.8490170836448669,\n",
            "    \"AI_confidence_difference\": 0.6980341672897339,\n",
            "    \"AI_probabilities\": {\n",
            "        \"Human-written\": 0.8490170836448669,\n",
            "        \"AI-generated\": 0.15098288655281067\n",
            "    },\n",
            "    \"AI_metrics\": {\n",
            "        \"character_count\": 21,\n",
            "        \"word_count\": 2,\n",
            "        \"average_word_length\": 10.5,\n",
            "        \"token_count\": 5,\n",
            "        \"max_token_length\": 512\n",
            "    },\n",
            "    \"AI_threshold_used\": 0.7\n",
            "}\n",
            "Row 81 processed in 39.39 seconds.\n",
            "\n",
            "Currently processing row: 82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/lib/python3.11/asyncio/base_events.py:1937: RuntimeWarning: coroutine 'save_progress' was never awaited\n",
            "  handle = None  # Needed to break cycles when an exception occurs.\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing result (JSON): {\n",
            "    \"question\": \"Q8. In an RSA cryptosystem, a particular A uses two prime numbers p = 17 and q =11 to generate her\\npublic and private keys. If the public key (e) of A is 7. Then the private key (d) of A is? (4 Marks)\\n\",\n",
            "    \"student_answer\": \"\\nAnswer= 23\",\n",
            "    \"expected_answer\": \"\",\n",
            "    \"marks\": 4.0,\n",
            "    \"score\": 3.0,\n",
            "    \"score_percentage\": 75.0,\n",
            "    \"feedback\": null,\n",
            "    \"similarity\": 0.2596970200538635,\n",
            "    \"similarity_percentage\": 25.97,\n",
            "    \"final_marks_obtained\": 3.1171655928933584,\n",
            "    \"final_marks_percentage\": 77.93,\n",
            "    \"student_word_count\": 2,\n",
            "    \"student_sentence_count\": 0,\n",
            "    \"plagiarism\": false,\n",
            "    \"average_score\": 0.0,\n",
            "    \"max_score\": 0,\n",
            "    \"max_score_url\": \"\",\n",
            "    \"plagiarism_data\": [\n",
            "        {\n",
            "            \"sentence\": \"\\nAnswer= 23\",\n",
            "            \"url\": null,\n",
            "            \"score\": 0.0,\n",
            "            \"plagiarism\": false\n",
            "        }\n",
            "    ],\n",
            "    \"AI_category\": \"Human-written\",\n",
            "    \"AI_flag\": false,\n",
            "    \"AI_percentage\": 11.66,\n",
            "    \"AI_confidence\": 0.8833768963813782,\n",
            "    \"AI_confidence_difference\": 0.7667537927627563,\n",
            "    \"AI_probabilities\": {\n",
            "        \"Human-written\": 0.8833768963813782,\n",
            "        \"AI-generated\": 0.11662309616804123\n",
            "    },\n",
            "    \"AI_metrics\": {\n",
            "        \"character_count\": 11,\n",
            "        \"word_count\": 2,\n",
            "        \"average_word_length\": 5.5,\n",
            "        \"token_count\": 6,\n",
            "        \"max_token_length\": 512\n",
            "    },\n",
            "    \"AI_threshold_used\": 0.7\n",
            "}\n",
            "Row 82 processed in 45.84 seconds.\n",
            "\n",
            "Currently processing row: 83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import asyncio\n",
        "from time import time\n",
        "\n",
        "# File paths\n",
        "input_file_path = '/content/drive/MyDrive/sses/cleaned_lsngjlxe_sses.xlsx'\n",
        "output_file_path = '/content/drive/MyDrive/sses/output_results.xlsx'\n",
        "\n",
        "# Parameters\n",
        "start_row = 80  # Start processing from row 81 (0-based index)\n",
        "retry_attempts = 3  # Number of retries for GetData\n",
        "\n",
        "# Load the input file\n",
        "df = pd.read_excel(input_file_path)\n",
        "\n",
        "# Load existing results if available\n",
        "try:\n",
        "    existing_df = pd.read_excel(output_file_path)\n",
        "    print(\"Existing results loaded.\")\n",
        "    last_saved_row = len(existing_df)\n",
        "except FileNotFoundError:\n",
        "    existing_df = pd.DataFrame(columns=df.columns.tolist() + ['evaluation_result_json'])\n",
        "    print(\"Output file not found. Starting fresh.\")\n",
        "    last_saved_row = 0\n",
        "\n",
        "# Function to save progress after processing a row\n",
        "async def save_progress(new_row_data):\n",
        "    global last_saved_row, existing_df, output_file_path\n",
        "\n",
        "    # Convert the new row data to a DataFrame\n",
        "    new_row_df = pd.DataFrame([new_row_data])\n",
        "\n",
        "    # Append the new row to the existing DataFrame\n",
        "    existing_df = pd.concat([existing_df, new_row_df], ignore_index=True)\n",
        "\n",
        "    # Save the entire DataFrame back to the file\n",
        "    existing_df.to_excel(output_file_path, index=False)\n",
        "\n",
        "    # Update the last saved row\n",
        "    last_saved_row += 1\n",
        "    print(f\"Row {last_saved_row} saved to drive.\")\n",
        "\n",
        "# Main processing function\n",
        "async def process_rows():\n",
        "    global existing_df\n",
        "\n",
        "    for index, row in df.iloc[start_row:].iterrows():\n",
        "        start_time = time()\n",
        "        print(f\"Currently processing row: {index + 1}\")\n",
        "        question = row['question_text']\n",
        "        student_answer = row['student_answer']\n",
        "        full_marks = row['full_marks']\n",
        "        expected_answer = \"\"  # No expected answer\n",
        "\n",
        "        # Retry mechanism for GetData\n",
        "        for attempt in range(retry_attempts):\n",
        "            try:\n",
        "                # Call GetData\n",
        "                evaluation_result = GetData(question, student_answer, expected_answer, full_marks)\n",
        "\n",
        "                # Add the evaluation result to the row\n",
        "                row_data = row.to_dict()\n",
        "                row_data['evaluation_result_json'] = json.dumps(evaluation_result, default=str)\n",
        "\n",
        "                # Save progress after processing the row\n",
        "                await save_progress(row_data)\n",
        "\n",
        "                # Print result\n",
        "                print(f\"Row {index + 1} processed and saved in {time() - start_time:.2f} seconds.\")\n",
        "                print(f\"Result JSON:\\n{json.dumps(evaluation_result, indent=4)}\\n\")\n",
        "                break  # Exit retry loop on success\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing row {index + 1}, attempt {attempt + 1}: {e}\")\n",
        "                if attempt == retry_attempts - 1:\n",
        "                    print(f\"Skipping row {index + 1} after {retry_attempts} attempts.\")\n",
        "                else:\n",
        "                    await asyncio.sleep(0.5)  # Short delay before retrying\n",
        "                continue\n",
        "\n",
        "    print(\"All rows processed and saved.\")\n",
        "\n",
        "# Run the async processing function\n",
        "await process_rows()"
      ],
      "metadata": {
        "id": "mQAGd0QMv1Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import re\n",
        "import math\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Global variables for models and tokenizers\n",
        "model_qwen = None\n",
        "tokenizer_qwen = None\n",
        "sentence_transformer = None\n",
        "models_loaded = False  # Flag to ensure models are loaded only once\n",
        "\n",
        "\n",
        "def load_models():\n",
        "    \"\"\"\n",
        "    Load global models and tokenizers.\n",
        "    This function should be called only once.\n",
        "    \"\"\"\n",
        "    global model_qwen, tokenizer_qwen, sentence_transformer, models_loaded\n",
        "\n",
        "    if not models_loaded:  # Load models only if not already loaded\n",
        "        print(\"Loading models...\")\n",
        "\n",
        "        # Load Qwen model\n",
        "        drive_path_qwen = \"/content/drive/MyDrive/sses/Qwen2.5-1.5B-Instruct\"\n",
        "        tokenizer_qwen = AutoTokenizer.from_pretrained(drive_path_qwen, local_files_only=True)\n",
        "        model_qwen = AutoModelForCausalLM.from_pretrained(drive_path_qwen, local_files_only=True).to(torch.device(\"cpu\"))\n",
        "\n",
        "        # Load SentenceTransformer for semantic similarity (MiniLM model)\n",
        "        drive_path_minilm = \"/content/drive/MyDrive/sses/all-MiniLM-L6-v2\"\n",
        "        sentence_transformer = SentenceTransformer(drive_path_minilm, device=\"cpu\")\n",
        "\n",
        "        models_loaded = True\n",
        "        print(\"Models loaded successfully.\")\n",
        "    else:\n",
        "        print(\"Models are already loaded.\")\n",
        "\n",
        "\n",
        "def extract_score_and_feedback(generated_text):\n",
        "    \"\"\"\n",
        "    Extract score and feedback from the generated text using regular expressions.\n",
        "\n",
        "    :param generated_text: The raw output from the model.\n",
        "    :return: A dictionary with \"score\" and \"feedback\".\n",
        "    \"\"\"\n",
        "    score_pattern = r'\"score\":\\s*([0-9.]+)'  # Matches numeric values for score\n",
        "    feedback_pattern = r'\"feedback\":\\s*\"([^\"]+)\"'  # Matches the feedback string\n",
        "\n",
        "    # Find matches\n",
        "    score_match = re.search(score_pattern, generated_text)\n",
        "    feedback_match = re.search(feedback_pattern, generated_text)\n",
        "\n",
        "    # Extract values\n",
        "    score = float(score_match.group(1)) if score_match else None  # Convert to float if match found\n",
        "    feedback = feedback_match.group(1) if feedback_match else None  # Extract feedback text if match found\n",
        "\n",
        "    return {\"score\": score, \"feedback\": feedback}\n",
        "\n",
        "\n",
        "def calculate_semantic_similarity(student_answer, expected_answer):\n",
        "    \"\"\"\n",
        "    Calculate the semantic similarity between the student's answer and the expected answer.\n",
        "\n",
        "    :param student_answer: The student's answer.\n",
        "    :param expected_answer: The expected correct answer.\n",
        "    :return: The semantic similarity score.\n",
        "    \"\"\"\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Encode both answers using the SentenceTransformer model\n",
        "    embeddings = sentence_transformer.encode([student_answer, expected_answer], convert_to_tensor=True)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity_score = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
        "    return similarity_score\n",
        "\n",
        "\n",
        "def GetData(question, student_answer, expected_answer, marks):\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    question = question or \"\"\n",
        "    student_answer = student_answer or \"\"\n",
        "    expected_answer = expected_answer or \"\"\n",
        "    marks = marks or 0\n",
        "\n",
        "    generated_text = generate_response(question, student_answer, marks)\n",
        "    result = extract_score_and_feedback(generated_text)\n",
        "    score = max(result.get('score', 0), 0)\n",
        "\n",
        "    similarity = max(calculate_semantic_similarity(student_answer, expected_answer) or 0, 0)\n",
        "    similarity_percentage = round(similarity * 100, 2)\n",
        "\n",
        "    log_score = math.log1p(score)\n",
        "    log_similarity = math.log1p(similarity)\n",
        "    weighted_score = (log_score * 0.5 + log_similarity * 0.5) * marks\n",
        "\n",
        "    max_allowed_marks = marks * 0.9\n",
        "    final_marks_obtained = min(weighted_score, max_allowed_marks)\n",
        "    final_marks_obtained = max(final_marks_obtained, 0)\n",
        "    final_marks_obtained = (final_marks_obtained + score) / 2\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"student_answer\": student_answer,\n",
        "        \"expected_answer\": expected_answer,\n",
        "        \"marks\": marks,\n",
        "        \"score\": score,\n",
        "        \"final_marks_obtained\": round(final_marks_obtained, 2),\n",
        "        \"similarity_percentage\": similarity_percentage,\n",
        "    }"
      ],
      "metadata": {
        "id": "9YQeRDTKo1Ys"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load models\n",
        "load_models()\n",
        "\n",
        "# Define sample inputs for testing\n",
        "question = \"What is a process in an operating system?\"\n",
        "student_answer = \"A process is a program that is running on a computer.\"\n",
        "expected_answer = \"A process is a program that is being executed by the operating system. It consists of the program code, current activity, and resources.\"\n",
        "marks = 10\n",
        "\n",
        "# Test GetData function\n",
        "result = GetData(question, student_answer, expected_answer, marks)\n",
        "\n",
        "# Print the result\n",
        "print(result)"
      ],
      "metadata": {
        "id": "uN8ZH8FZpFBh",
        "outputId": "3bf74858-6aaf-44c2-9ce1-dc015ed79654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models...\n",
            "Models loaded successfully.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'>' not supported between instances of 'int' and 'NoneType'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-29c496889529>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Test GetData function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGetData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_answer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_answer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Print the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-0eb2cac777be>\u001b[0m in \u001b[0;36mGetData\u001b[0;34m(question, student_answer, expected_answer, marks)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_answer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_score_and_feedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_semantic_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_answer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_answer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'int' and 'NoneType'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model's response directly\n",
        "question = \"What is a process in operating systems?\"\n",
        "student_answer = \"A process is a program in execution.\"\n",
        "marks = 5\n",
        "\n",
        "def generate_response(question, student_answer, marks):\n",
        "    \"\"\"\n",
        "    Generate a response using the Qwen model.\n",
        "\n",
        "    :param question: The input question.\n",
        "    :param student_answer: The input student's answer.\n",
        "    :return: The generated text response.\n",
        "    \"\"\"\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Prepare the input text\n",
        "    input_text = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {student_answer}\n",
        "    Evaluate and provide these two fields **score** and **feedback** based on this answer where full marks is {marks} and can be given for the most precise and best answer, and 0 for wrong or irrelevant answers. Respond only with these two fields in JSON format.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    input_ids = tokenizer_qwen.encode(input_text, return_tensors=\"pt\").to(torch.device(\"cpu\"))\n",
        "\n",
        "    # Dynamically set the maximum number of new tokens\n",
        "    max_allowed_tokens = model_qwen.config.max_position_embeddings  # Total token limit of the model\n",
        "    input_token_count = len(input_ids[0])\n",
        "    max_new_tokens = max(50, max_allowed_tokens - input_token_count)  # Minimum 50 new tokens\n",
        "\n",
        "    # Generate the response\n",
        "    output_ids = model_qwen.generate(\n",
        "        input_ids,\n",
        "        max_length=input_token_count + max_new_tokens,  # Total length: input + generated tokens\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,  # Avoid repetition\n",
        "        early_stopping=True,    # Stop when the output is complete\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated output\n",
        "    return tokenizer_qwen.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# Generate response using the model\n",
        "generated_text = generate_response(question, student_answer, marks)\n",
        "\n",
        "# Print the generated response\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "2LtxMiG_phH0",
        "outputId": "444e30d3-e197-465c-dcae-91f46c306a4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Question: What is a process in operating systems?\n",
            "    Answer: A process is a program in execution.\n",
            "    Evaluate and provide these two fields **score** and **feedback** based on this answer where full marks is 5 and can be given for the most precise and best answer, and 0 for wrong or irrelevant answers. Respond only with these two fields in JSON format.\n",
            "     ```json\n",
            "     {\"score\": 3, \"feedback\": \"The answer is partially correct as it defines a 'process' but does not specify that it's in 'execution'. However, it correctly identifies processes as 'program(s) in operation', which aligns closely with the question.\"}\n",
            "     ```\n",
            "     The above output will update your score and feedback to reflect the evaluation of the provided response.\n",
            "\n",
            "```json \n",
            "{\"score\" : 2, \n",
            "\"feedback\" :\"The definition provided is accurate but lacks specificity regarding the context (e.g., user interface, database operations). It could benefit from clarifying if we are referring to multi-threaded or single-thread processes. Additionally, providing more examples would enhance understanding.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(question, student_answer, marks):\n",
        "    \"\"\"\n",
        "    Generate a response using the Qwen model.\n",
        "\n",
        "    :param question: The input question.\n",
        "    :param student_answer: The input student's answer.\n",
        "    :return: The generated text response.\n",
        "    \"\"\"\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Prepare the input text\n",
        "    input_text = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {student_answer}\n",
        "    Evaluate and provide these two fields **score** and **feedback** based on this answer where full marks is {marks} and can be given for the most precise and best answer, and 0 for wrong or irrelevant answers. Respond only with these two fields in JSON format.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    input_ids = tokenizer_qwen.encode(input_text, return_tensors=\"pt\").to(torch.device(\"cpu\"))\n",
        "\n",
        "    # Dynamically set the maximum number of new tokens\n",
        "    max_allowed_tokens = model_qwen.config.max_position_embeddings  # Total token limit of the model\n",
        "    input_token_count = len(input_ids[0])\n",
        "    max_new_tokens = max(50, max_allowed_tokens - input_token_count)  # Minimum 50 new tokens\n",
        "\n",
        "    # Generate the response (only 1 sequence)\n",
        "    output_ids = model_qwen.generate(\n",
        "        input_ids,\n",
        "        max_length=input_token_count + max_new_tokens,  # Total length: input + generated tokens\n",
        "        num_return_sequences=1,  # Ensure only 1 response\n",
        "        no_repeat_ngram_size=2,  # Avoid repetition\n",
        "        early_stopping=False,    # Remove early stopping warning\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated output\n",
        "    return tokenizer_qwen.decode(output_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "weRP0Ixzq1lM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "question = \"What is a process in operating systems?\"\n",
        "student_answer = \"\"\"\n",
        "A process is a program that is in execution. A process can be defined as a sequence of instructions that are executed by the CPU.\n",
        "A program, in its static form, is a collection of instructions, whereas a process refers to the program in action.\n",
        "Each process has its own memory space and system resources, and it is managed by the operating system. The operating system\n",
        "allocates resources to the process and manages its execution through process scheduling. Processes are fundamental\n",
        "to the operation of a computer system, enabling multitasking and allowing multiple programs to run simultaneously.\n",
        "Each process consists of a program counter, registers, and a stack, which together allow the operating system to keep track\n",
        "of the state of the process. Processes can be classified as either user-level processes or kernel-level processes,\n",
        "depending on whether they interact directly with the user or operate in the background of the system.\n",
        "In multi-threaded applications, a single process can have multiple threads, which are smaller units of execution\n",
        "that share the same memory space. Processes also communicate with each other via inter-process communication (IPC)\n",
        "mechanisms, such as message passing or shared memory. As a process executes, it may transition through various states,\n",
        "including ready, running, waiting, and terminated, depending on its interaction with the system and the availability of resources.\n",
        "\"\"\"\n",
        "marks = 5\n",
        "\n",
        "# Generate response using the model\n",
        "generated_text = generate_response(question, student_answer, marks)\n",
        "\n",
        "# Print the generated response\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "zynEI-ykq7Lt",
        "outputId": "0a7b5949-3aae-43bd-8b0a-f66f7f6e21f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"score\": 2, \"feedback\": \"The explanation provides accurate definitions but could benefit from more detailed examples to enhance understanding.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def generate_response(question, student_answer, marks):\n",
        "    \"\"\"\n",
        "    Generate a response using the Qwen model.\n",
        "\n",
        "    :param question: The input question.\n",
        "    :param student_answer: The input student's answer.\n",
        "    :return: The generated text response.\n",
        "    \"\"\"\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Prepare the input text\n",
        "    input_text = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {student_answer}\n",
        "    Evaluate and provide these two fields **score** and **feedback** based on this answer where full marks is {marks} and can be given for the most precise and best answer, and 0 for wrong or irrelevant answers. Respond only with these two fields in JSON format.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the input\n",
        "    input_ids = tokenizer_qwen.encode(input_text, return_tensors=\"pt\").to(torch.device(\"cpu\"))\n",
        "\n",
        "    # Dynamically set the maximum number of new tokens\n",
        "    max_allowed_tokens = model_qwen.config.max_position_embeddings  # Total token limit of the model\n",
        "    input_token_count = len(input_ids[0])\n",
        "    max_new_tokens = max(50, max_allowed_tokens - input_token_count)  # Minimum 50 new tokens\n",
        "\n",
        "    # Generate the response (only 1 sequence)\n",
        "    output_ids = model_qwen.generate(\n",
        "        input_ids,\n",
        "        max_length=input_token_count + max_new_tokens,  # Total length: input + generated tokens\n",
        "        num_return_sequences=1,  # Ensure only 1 response\n",
        "        no_repeat_ngram_size=2,  # Avoid repetition\n",
        "        early_stopping=False,    # Remove early stopping warning\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated output\n",
        "    generated_text = tokenizer_qwen.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the JSON part\n",
        "    try:\n",
        "        start_idx = generated_text.index(\"{\")\n",
        "        end_idx = generated_text.rindex(\"}\") + 1\n",
        "        return generated_text[start_idx:end_idx]\n",
        "    except ValueError:\n",
        "        return generated_text  # Fallback if JSON is not found"
      ],
      "metadata": {
        "id": "I9xpUi77r8PQ"
      },
      "execution_count": 32,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4054a3f43a254da889c0f385a726f039": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98b4b4e6953b451b9f54a664f540f5fe",
              "IPY_MODEL_9b214ea429f149dfbb9d835a5aa15c35",
              "IPY_MODEL_b1fb64a1db84488595a85c7aa1d914fd"
            ],
            "layout": "IPY_MODEL_97fd35b186474fde9e8c21ca6a59c744"
          }
        },
        "98b4b4e6953b451b9f54a664f540f5fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3ab126bc08a44828cb8c72fd4608737",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_30642a30289445c384fdfc62f29cd92d",
            "value": "tokenizer_config.json:â€‡100%"
          }
        },
        "9b214ea429f149dfbb9d835a5aa15c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab449d4e17974ec081d0c955a19e51e9",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e39f17c86424fafba1defe4ac585f5a",
            "value": 25
          }
        },
        "b1fb64a1db84488595a85c7aa1d914fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a3740f5eb6f4099a9b09984ee4d5cb1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1f8a7ddb80154d07904560c9317785b4",
            "value": "â€‡25.0/25.0â€‡[00:00&lt;00:00,â€‡1.52kB/s]"
          }
        },
        "97fd35b186474fde9e8c21ca6a59c744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3ab126bc08a44828cb8c72fd4608737": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30642a30289445c384fdfc62f29cd92d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab449d4e17974ec081d0c955a19e51e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e39f17c86424fafba1defe4ac585f5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a3740f5eb6f4099a9b09984ee4d5cb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f8a7ddb80154d07904560c9317785b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7274285987d74e31a7dcc1adf86daa9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff5a0530b13e4dac8280c86293a8190d",
              "IPY_MODEL_de2637e3dc7d462386f79512944a7032",
              "IPY_MODEL_c7ae3710c8074a00943c445b4de8d5a1"
            ],
            "layout": "IPY_MODEL_368706dd3df2482181c17ee3b3835192"
          }
        },
        "ff5a0530b13e4dac8280c86293a8190d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf85a8ce87f04ee8acf84e171352de84",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_65276d7e6d5e4de7b454794156285663",
            "value": "config.json:â€‡100%"
          }
        },
        "de2637e3dc7d462386f79512944a7032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f55026b61a44272994c418a6a59db66",
            "max": 624,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a10c0168f4fd4034a47a5728fef4a9ee",
            "value": 624
          }
        },
        "c7ae3710c8074a00943c445b4de8d5a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f1aa59fd7ea42f4ac246f23c9109a2c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6257942ecca6403e82d87e1ed913e5a8",
            "value": "â€‡624/624â€‡[00:00&lt;00:00,â€‡38.9kB/s]"
          }
        },
        "368706dd3df2482181c17ee3b3835192": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf85a8ce87f04ee8acf84e171352de84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65276d7e6d5e4de7b454794156285663": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f55026b61a44272994c418a6a59db66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a10c0168f4fd4034a47a5728fef4a9ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f1aa59fd7ea42f4ac246f23c9109a2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6257942ecca6403e82d87e1ed913e5a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "751ba946e23f421493d54f66cd474302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27b580787091416c8c3e4b9fccfffa05",
              "IPY_MODEL_517909cf302b4475a51a291c13865eac",
              "IPY_MODEL_5178126832d94b1e9bdbc9bb963185ef"
            ],
            "layout": "IPY_MODEL_8a1a0d957a6f467e949e8f87a3d7267e"
          }
        },
        "27b580787091416c8c3e4b9fccfffa05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57f48e86cece4252b9fa3331336d17b2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a303b26e83544b05a463a4f2d2991bee",
            "value": "vocab.json:â€‡100%"
          }
        },
        "517909cf302b4475a51a291c13865eac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31926ab9781747039443c4cf684f6492",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_091d07595c404d2ea29d0fae59544d02",
            "value": 898823
          }
        },
        "5178126832d94b1e9bdbc9bb963185ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58bb11cbf3ea4e46a5010d53cff62e7e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f1d3294d6d6e412ebd108320be699942",
            "value": "â€‡899k/899kâ€‡[00:00&lt;00:00,â€‡7.70MB/s]"
          }
        },
        "8a1a0d957a6f467e949e8f87a3d7267e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57f48e86cece4252b9fa3331336d17b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a303b26e83544b05a463a4f2d2991bee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31926ab9781747039443c4cf684f6492": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "091d07595c404d2ea29d0fae59544d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58bb11cbf3ea4e46a5010d53cff62e7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1d3294d6d6e412ebd108320be699942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8448f12e45045f183fed5cafc242d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c1de39b68a74b8c826f7577f5807685",
              "IPY_MODEL_35f2e6850156425fbf20909ae55c0bf8",
              "IPY_MODEL_41f9a65643c844df963104cda591fca3"
            ],
            "layout": "IPY_MODEL_7d488776125347a9a22a29b552f8d2d8"
          }
        },
        "9c1de39b68a74b8c826f7577f5807685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_456e196b33e34842b36a853a8283a528",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ce26b9bdbd7042368c705c9b68926fa2",
            "value": "merges.txt:â€‡100%"
          }
        },
        "35f2e6850156425fbf20909ae55c0bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e32f86db94e2413c8d3449881f47413a",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe8dc9b2440d43bf8f9f06c06fe7e64a",
            "value": 456318
          }
        },
        "41f9a65643c844df963104cda591fca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b52df9d70f243e4bdb23b421afcea7f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_df0aa19fa9d54acb9e253ef5ed8fb1fa",
            "value": "â€‡456k/456kâ€‡[00:00&lt;00:00,â€‡14.1MB/s]"
          }
        },
        "7d488776125347a9a22a29b552f8d2d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "456e196b33e34842b36a853a8283a528": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce26b9bdbd7042368c705c9b68926fa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e32f86db94e2413c8d3449881f47413a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe8dc9b2440d43bf8f9f06c06fe7e64a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b52df9d70f243e4bdb23b421afcea7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df0aa19fa9d54acb9e253ef5ed8fb1fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92aec70dfee0486ea27ce00503432e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd1e453bbf4943a98fec5d5063a521e7",
              "IPY_MODEL_699569dcca0a4f3992d106696877f837",
              "IPY_MODEL_0492d2091adc4e4db367d58deb2e8ea0"
            ],
            "layout": "IPY_MODEL_ee20871b358e4b9086f575555591dff2"
          }
        },
        "cd1e453bbf4943a98fec5d5063a521e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbc4d5b75bd0460faaf146658bbacb9e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_537087f9e22648209ca533ad53dec3f7",
            "value": "tokenizer.json:â€‡100%"
          }
        },
        "699569dcca0a4f3992d106696877f837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9278d2848d2145d8bd6e6af81fd1c476",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92821faaf6834591bfa619a90f14643f",
            "value": 1355863
          }
        },
        "0492d2091adc4e4db367d58deb2e8ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d318fbff86a414999bf5f557fce6291",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4f4f9fd677d44921b284b9f829941ecc",
            "value": "â€‡1.36M/1.36Mâ€‡[00:00&lt;00:00,â€‡7.70MB/s]"
          }
        },
        "ee20871b358e4b9086f575555591dff2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbc4d5b75bd0460faaf146658bbacb9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "537087f9e22648209ca533ad53dec3f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9278d2848d2145d8bd6e6af81fd1c476": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92821faaf6834591bfa619a90f14643f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d318fbff86a414999bf5f557fce6291": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f4f9fd677d44921b284b9f829941ecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31b7a3c43d354884a9e211de6520f549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bfb22529bf1b4b12853ad54542daf703",
              "IPY_MODEL_de81bff4716146efb56168d48d37eccb",
              "IPY_MODEL_d6cb201b1e59470b98fcf02e0d741a37"
            ],
            "layout": "IPY_MODEL_69dafaac7a9a4ba49405d3554efb8f65"
          }
        },
        "bfb22529bf1b4b12853ad54542daf703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7636a86187ba417b99194487b363ea17",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_76c70e48f2a84fc9b7f5af62e26fb804",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "de81bff4716146efb56168d48d37eccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a131639a4284f6391b793ea1c39c694",
            "max": 500975390,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_730397abcd82405f954dd926551b3ba6",
            "value": 500975390
          }
        },
        "d6cb201b1e59470b98fcf02e0d741a37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a6edb02187e46069afe8f38edd6948b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6eefc4268ab844b4b4186a878756d916",
            "value": "â€‡501M/501Mâ€‡[00:06&lt;00:00,â€‡82.7MB/s]"
          }
        },
        "69dafaac7a9a4ba49405d3554efb8f65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7636a86187ba417b99194487b363ea17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76c70e48f2a84fc9b7f5af62e26fb804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a131639a4284f6391b793ea1c39c694": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "730397abcd82405f954dd926551b3ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a6edb02187e46069afe8f38edd6948b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eefc4268ab844b4b4186a878756d916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}