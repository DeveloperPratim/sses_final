{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPnnQ5nxfURYofchFLXVY2K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeveloperPratim/sses_final/blob/main/Mistrial%20AIUntitled32.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofJCIBSHJ1Sz",
        "outputId": "608dc873-2136-45ef-df52-fb0f42d9a815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git@72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
            "  Cloning https://github.com/huggingface/transformers.git (to revision 72958fcd3c98a7afdc61f953aa58c544ebda2f79) to /tmp/pip-req-build-q1byzgyo\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-q1byzgyo\n",
            "  Running command git rev-parse -q --verify 'sha^72958fcd3c98a7afdc61f953aa58c544ebda2f79'\n",
            "  Running command git fetch -q https://github.com/huggingface/transformers.git 72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
            "  Running command git checkout -q 72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (2.32.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.0.dev0)\n",
            "  Downloading tokenizers-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.34.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (4.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.0.dev0)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0.dev0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.34.0.dev0) (2024.12.14)\n",
            "Downloading tokenizers-0.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.34.0.dev0-py3-none-any.whl size=7746278 sha256=e49109062f6a486f117c1860606bee94ca4b8aa976303df2a1ee591f3b39c5d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/7e/b3/04336a04924aa0de029f81a87788444ff8202f8eeeb389c534\n",
            "Successfully built transformers\n",
            "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.27.1\n",
            "    Uninstalling huggingface-hub-0.27.1:\n",
            "      Successfully uninstalled huggingface-hub-0.27.1\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.47.1\n",
            "    Uninstalling transformers-4.47.1:\n",
            "      Successfully uninstalled transformers-4.47.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.3.1 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.34.0.dev0 which is incompatible.\n",
            "peft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "accelerate 1.2.1 requires huggingface-hub>=0.21.0, but you have huggingface-hub 0.17.3 which is incompatible.\n",
            "diffusers 0.32.1 requires huggingface-hub>=0.23.2, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.17.3 tokenizers-0.14.1 transformers-4.34.0.dev0\n",
            "Collecting git+https://github.com/casper-hansen/AutoAWQ.git@1c5ccc791fa2cb0697db3b4070df1813f1736208\n",
            "  Cloning https://github.com/casper-hansen/AutoAWQ.git (to revision 1c5ccc791fa2cb0697db3b4070df1813f1736208) to /tmp/pip-req-build-uvq5vs60\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/casper-hansen/AutoAWQ.git /tmp/pip-req-build-uvq5vs60\n",
            "  Running command git rev-parse -q --verify 'sha^1c5ccc791fa2cb0697db3b4070df1813f1736208'\n",
            "  Running command git fetch -q https://github.com/casper-hansen/AutoAWQ.git 1c5ccc791fa2cb0697db3b4070df1813f1736208\n",
            "  Running command git checkout -q 1c5ccc791fa2cb0697db3b4070df1813f1736208\n",
            "  Resolved https://github.com/casper-hansen/AutoAWQ.git to commit 1c5ccc791fa2cb0697db3b4070df1813f1736208\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git@72958fcd3c98a7afdc61f953aa58c544ebda2f79\n",
        "\n",
        "!pip install git+https://github.com/casper-hansen/AutoAWQ.git@1c5ccc791fa2cb0697db3b4070df1813f1736208"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autoawq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8rAYKz_J82b",
        "outputId": "448299e4-ced2-48ad-b856-0b9263626f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autoawq\n",
            "  Downloading autoawq-0.2.7.post3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from autoawq) (2.5.1+cu121)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (from autoawq) (3.1.0)\n",
            "Collecting transformers>=4.45.0 (from autoawq)\n",
            "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from autoawq) (0.14.1)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from autoawq) (4.12.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from autoawq) (1.2.1)\n",
            "Collecting datasets>=2.20 (from autoawq)\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting zstandard (from autoawq)\n",
            "  Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting huggingface_hub>=0.26.5 (from autoawq)\n",
            "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.20->autoawq)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (4.67.1)\n",
            "Collecting xxhash (from datasets>=2.20->autoawq)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.20->autoawq)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.20->autoawq)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (3.11.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.20->autoawq) (6.0.2)\n",
            "INFO: pip is looking at multiple versions of tokenizers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tokenizers>=0.12.1 (from autoawq)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->autoawq) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->autoawq) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->autoawq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->autoawq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->autoawq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->autoawq) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->autoawq) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->autoawq) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->autoawq) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->autoawq) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->autoawq) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->autoawq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->autoawq) (12.1.105)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->autoawq) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.2.0->autoawq) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.2.0->autoawq) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.45.0->autoawq) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.45.0->autoawq) (0.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->autoawq) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.20->autoawq) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.20->autoawq) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.2.0->autoawq) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.20->autoawq) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.20->autoawq) (1.17.0)\n",
            "Downloading autoawq-0.2.7.post3-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.7/450.7 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: zstandard, xxhash, fsspec, dill, multiprocess, huggingface_hub, tokenizers, transformers, datasets, autoawq\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.17.3\n",
            "    Uninstalling huggingface-hub-0.17.3:\n",
            "      Successfully uninstalled huggingface-hub-0.17.3\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.14.1\n",
            "    Uninstalling tokenizers-0.14.1:\n",
            "      Successfully uninstalled tokenizers-0.14.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.34.0.dev0\n",
            "    Uninstalling transformers-4.34.0.dev0:\n",
            "      Successfully uninstalled transformers-4.34.0.dev0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed autoawq-0.2.7.post3 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 huggingface_hub-0.27.1 multiprocess-0.70.16 tokenizers-0.21.0 transformers-4.48.0 xxhash-3.5.0 zstandard-0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import hashlib\n",
        "import json\n",
        "import random\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Ensure NLTK resources are downloaded for sentence tokenization\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    print(\"Downloading 'punkt' package...\")\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "def get_useragent():\n",
        "    _useragent_list = [\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.62',\n",
        "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0'\n",
        "    ]\n",
        "    return random.choice(_useragent_list)\n",
        "\n",
        "def google_search(query):\n",
        "    links = []\n",
        "    try:\n",
        "        headers = {\"User-Agent\": get_useragent()}\n",
        "        params = {\"q\": f'\"{query}\"', \"num\": 3, \"hl\": 'en'}\n",
        "\n",
        "        # Perform the Google search request\n",
        "        response = requests.get(\"https://www.google.com/search\", headers=headers, params=params, timeout=5)\n",
        "        response.raise_for_status()  # Raises an error for bad responses\n",
        "\n",
        "        # Parse the response content with BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract URLs from the search result blocks (avoiding ads, etc.)\n",
        "\n",
        "        try:\n",
        "            for result in soup.find_all('h3'):\n",
        "                link = result.find_parent('a')\n",
        "                if link:\n",
        "                    link_text = link.get('href')\n",
        "                    links.append(link_text)\n",
        "        except:\n",
        "            for link_tag in soup.select('div.yuRUbf a'):\n",
        "                link = link_tag.get('href')\n",
        "                if link:\n",
        "                    links.append(link)\n",
        "\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error during the search request: {e}\")\n",
        "\n",
        "    return links\n",
        "\n",
        "def fetch_web_content(url: str) -> str:\n",
        "    \"\"\"Fetch and clean the main content of a webpage.\"\"\"\n",
        "    try:\n",
        "        headers = {\"User-Agent\": get_useragent()}\n",
        "        response = requests.get(url, headers=headers, timeout=5)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        paragraphs = soup.find_all(\"p\")\n",
        "        return \" \".join([para.get_text() for para in paragraphs])\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to fetch {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def get_shingles(text: str, k: int = 5) -> set:\n",
        "    \"\"\"Generate k-shingles (sets of k consecutive words) for a given text.\"\"\"\n",
        "    words = text.split()\n",
        "    shingles = set()\n",
        "    for i in range(len(words) - k + 1):\n",
        "        shingle = \" \".join(words[i:i + k])\n",
        "        shingle_hash = hashlib.md5(shingle.encode(\"utf-8\")).hexdigest()\n",
        "        shingles.add(shingle_hash)\n",
        "    return shingles\n",
        "\n",
        "def similarity1(set1, set2):\n",
        "    # Check if either set is empty to prevent division by zero\n",
        "    if len(set1) == 0 or len(set2) == 0:\n",
        "        return 0.0\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    return (intersection / len(set1)) * 100\n",
        "\n",
        "def calculate_jaccard_similarity(shingles1: set, shingles2: set) -> float:\n",
        "    \"\"\"Calculate Jaccard similarity between two sets of shingles.\"\"\"\n",
        "    intersection = shingles1.intersection(shingles2)\n",
        "    union = shingles1.union(shingles2)\n",
        "\n",
        "    # Avoid division by zero if both sets are empty\n",
        "    if len(union) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return len(intersection) / len(union)\n",
        "\n",
        "def check_sentence_plagiarism(sentence: str) -> Dict[str, Any]:\n",
        "    \"\"\"Check plagiarism for a single sentence by searching and comparing content.\"\"\"\n",
        "    result = {\"sentence\": sentence, \"matches\": []}\n",
        "    urls = google_search(sentence)\n",
        "\n",
        "    for url in urls:\n",
        "        content = fetch_web_content(url)\n",
        "        if content:\n",
        "            original_shingles = get_shingles(sentence)\n",
        "            content_shingles = get_shingles(content)\n",
        "            # similarity = calculate_jaccard_similarity(original_shingles, content_shingles)\n",
        "            similarity = similarity1(original_shingles, content_shingles)\n",
        "            result[\"matches\"].append({\"url\": url, \"score\": similarity})\n",
        "\n",
        "    # Sort matches by score in descending order and take the highest\n",
        "    if result[\"matches\"]:\n",
        "        result[\"matches\"].sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "        highest_match = result[\"matches\"][0]\n",
        "        result[\"highest_match\"] = {\"url\": highest_match[\"url\"], \"score\": highest_match[\"score\"]}\n",
        "    else:\n",
        "        result[\"highest_match\"] = {\"url\": None, \"score\": 0.0}\n",
        "\n",
        "    return result\n",
        "\n",
        "def plagiarism_checker(text: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Run plagiarism check on each sentence in the input text.\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    results = [check_sentence_plagiarism(sentence) for sentence in sentences]\n",
        "    return get_score(results)\n",
        "\n",
        "def get_score(data):\n",
        "    # Merging sentences by URL and score\n",
        "    result = []\n",
        "    total_score = 0\n",
        "    max_score = 0\n",
        "    max_score_url = \"\"\n",
        "    total_sentences = len(data)\n",
        "\n",
        "    for entry in data:\n",
        "        url = entry['highest_match']['url']\n",
        "        score = entry['highest_match']['score']\n",
        "        sentence = entry['sentence']\n",
        "\n",
        "        # Check if score is greater than 40 for plagiarism flag\n",
        "        plagiarism = True if score > 40 else False\n",
        "\n",
        "        # Add score to total score\n",
        "        total_score += score\n",
        "\n",
        "        # Track maximum score and corresponding URL\n",
        "        if score > max_score:\n",
        "            max_score = score\n",
        "            max_score_url = url\n",
        "\n",
        "        # Check if we should merge with the last entry\n",
        "        if result and result[-1]['url'] == url and result[-1]['score'] == score:\n",
        "            result[-1]['sentence'] += \" \" + sentence  # Append the sentence\n",
        "        else:\n",
        "            # Add a new entry with plagiarism flag\n",
        "            result.append({\n",
        "                'sentence': sentence,\n",
        "                'url': url,\n",
        "                'score': score,\n",
        "                'plagiarism': plagiarism\n",
        "            })\n",
        "\n",
        "    # Calculate average score\n",
        "    average_score = total_score / total_sentences if total_sentences else 0\n",
        "\n",
        "    # Create the final structured response\n",
        "    response = {\n",
        "        \"plagiarism\": any(entry['plagiarism'] for entry in result),\n",
        "        \"average_score\": average_score,\n",
        "        \"max_score\": max_score,\n",
        "        \"max_score_url\": max_score_url,\n",
        "        \"data\": result\n",
        "    }\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "# input_text = \"\"\"\n",
        "# An Operating System can be defined as an interface between user and hardware. It is responsible for the execution of all the processes, Resource Allocation, CPU ...\"\"\"\n",
        "# results_plagiarism= plagiarism_checker(input_text)\n",
        "# print(json.dumps(results_plagiarism, indent=2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def detect_ai_generated_text_advanced_v3(\n",
        "    text,\n",
        "    model_name=\"roberta-base-openai-detector\",\n",
        "    threshold=0.5,\n",
        "    max_length=512\n",
        "):\n",
        "    \"\"\"\n",
        "    Advanced detection of AI-generated text with detailed metrics, AI flag, and percentage.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): Input text to analyze.\n",
        "        model_name (str): Pre-trained model to use for detection.\n",
        "        threshold (float): Confidence threshold for categorization (default: 0.5).\n",
        "        max_length (int): Maximum length for tokenization (default: 512).\n",
        "\n",
        "    Returns:\n",
        "        dict: Detailed output with categorization, confidence, metrics, AI flag, and percentage.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load tokenizer and model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "        # Tokenize and prepare text\n",
        "        tokens = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "        token_count = len(tokens[\"input_ids\"][0])\n",
        "\n",
        "        # Inference\n",
        "        outputs = model(**tokens)\n",
        "        probabilities = torch.softmax(outputs.logits, dim=1).detach().numpy()[0]\n",
        "        ai_confidence = probabilities[1]\n",
        "        human_confidence = probabilities[0]\n",
        "        category = \"AI-generated\" if ai_confidence > threshold else \"Human-written\"\n",
        "\n",
        "        # AI flag and percentage\n",
        "        is_ai = ai_confidence > threshold\n",
        "        ai_percentage = round(ai_confidence * 100, 2)\n",
        "\n",
        "        # Additional metrics\n",
        "        char_count = len(text)\n",
        "        word_count = len(text.split())\n",
        "        avg_word_length = char_count / word_count if word_count > 0 else 0\n",
        "        confidence_diff = abs(ai_confidence - human_confidence)\n",
        "\n",
        "        return {\n",
        "            \"category\": category,\n",
        "            \"AI\": is_ai,\n",
        "            \"AI_Percentage\": ai_percentage,\n",
        "            \"confidence\": ai_confidence if is_ai else human_confidence,\n",
        "            \"confidence_difference\": confidence_diff,\n",
        "            \"probabilities\": {\n",
        "                \"Human-written\": human_confidence,\n",
        "                \"AI-generated\": ai_confidence\n",
        "            },\n",
        "            \"metrics\": {\n",
        "                \"character_count\": char_count,\n",
        "                \"word_count\": word_count,\n",
        "                \"average_word_length\": round(avg_word_length, 2),\n",
        "                \"token_count\": token_count,\n",
        "                \"max_token_length\": max_length\n",
        "            },\n",
        "            \"threshold_used\": threshold\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "# Example usage\n",
        "#text = \"In a world driven by rapid technological advancement, artificial intelligence has emerged as a transformative force. From revolutionizing healthcare with predictive diagnostics to enhancing efficiency in industries through automation, AI continues to redefine the boundaries of what is possible. However, this unprecedented growth also poses ethical dilemmas, emphasizing the need for responsible innovation to ensure these technologies benefit humanity as a whole.\"\n",
        "#result_AI = detect_ai_generated_text_advanced_v3(text, threshold=0.7)\n",
        "#print(result)\n",
        "#print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOhdWdEMKAh0",
        "outputId": "99bc359c-a936-45d3-8195-b30d7839bedc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 'punkt' package...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "import math\n",
        "import torch\n",
        "from awq import AutoAWQForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load model\n",
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "if 'model' not in globals():\n",
        "    model = AutoAWQForCausalLM.from_quantized(\n",
        "        model_name_or_path,\n",
        "        fuse_layers=True,\n",
        "        trust_remote_code=False,\n",
        "        safetensors=True\n",
        "    ).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
        "\n",
        "models_loaded = True\n",
        "\n",
        "# Function to evaluate the student's answer\n",
        "def evaluate_answer(question, answer, full_marks=5, expected_answer=None):\n",
        "    # Basic Information\n",
        "    prompt = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {answer}\n",
        "    Full Marks: {full_marks}\n",
        "    \"\"\"\n",
        "\n",
        "    if expected_answer:\n",
        "        prompt += f\"Expected Answer: {expected_answer}\\n\"\n",
        "\n",
        "    # Evaluation instructions\n",
        "    prompt += \"\"\"\n",
        "    Evaluate based on strictly: Correctness of answer, Completeness of answer, Clarity, Depth, and Relevance of answer , if answer not according to asked question or giving nearly similar answer just give it a 0 marks.\n",
        "\n",
        "    Return JSON with:\n",
        "    - marks_obtained\n",
        "    - feedback  ( detailed overall feedback )\n",
        "    - strengths ( detailed positive aspects )\n",
        "    - areas_for_improvemen ( for getting full marks)\n",
        "    - clarity_score (1-10)\n",
        "    - depth_score (1-10)\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the prompt for the model\n",
        "    tokens = tokenizer(prompt, return_tensors='pt').input_ids.to(device)\n",
        "\n",
        "    # Generate the model's response with the given prompt\n",
        "    generation_output = model.generate(\n",
        "        tokens, do_sample=True, temperature=0.7, top_p=0.95, top_k=40, max_new_tokens=512\n",
        "    )\n",
        "\n",
        "    # Decode the output from the model\n",
        "    response = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "def round_based_on_fraction(number):\n",
        "    # Get the fractional part of the number\n",
        "    fractional_part = number - math.floor(number)\n",
        "\n",
        "    # If the fractional part is greater than 0.5, round up, else round down\n",
        "    if fractional_part > 0.5:\n",
        "        return math.ceil(number)  # Round up\n",
        "    else:\n",
        "        return math.floor(number)  # Round down\n",
        "\n",
        "\n",
        "\n",
        "def GetData(question, student_answer, expected_answer, marks):\n",
        "    if not models_loaded:\n",
        "        raise RuntimeError(\"Models are not loaded. Call `load_models()` first.\")\n",
        "\n",
        "    # Ensure non-empty inputs\n",
        "    student_answer = student_answer or \"\"  # Default to empty string if None or empty\n",
        "    expected_answer = expected_answer or \"\"  # Default to empty string if None or empty\n",
        "    marks = marks or 0  # Default to 0 if marks are None or empty\n",
        "    question = question or \"\"  # Default to empty string if None or empty\n",
        "\n",
        "    # Generate response using the model\n",
        "    generated_text = evaluate_answer(question, student_answer, marks, expected_answer)\n",
        "\n",
        "    # Extract score and feedback from the model's output\n",
        "    result = extract_score_and_feedback(generated_text)\n",
        "\n",
        "    # Handle missing or None values gracefully using .get() with default values\n",
        "    score = result.get('marks_obtained', 0)  # Default to 0 if score is None or not found\n",
        "    score_old = score\n",
        "    feedback = result.get('feedback', \"\")  # Default to an empty string if feedback is None or not found\n",
        "    strengths = result.get('strengths', \"\")  # Default to an empty string if strengths are None or not found\n",
        "    areas_for_improvement = result.get('areas_for_improvement', \"\")  # Default to an empty string if areas are None or not found\n",
        "    clarity_score = result.get('clarity_score', 0)  # Default to 0 if clarity score is None or not found\n",
        "    depth_score = result.get('depth_score', 0)  # Default to 0 if depth score is None or not found\n",
        "\n",
        "    # Ensure score is not None before using it in arithmetic operations\n",
        "    score = score if score is not None else 0\n",
        "\n",
        "    # Logarithmic scaling of score\n",
        "    log_score = math.log(score + 1) if score is not None else 0  # Logarithm of score (score + 1 to avoid log(0))\n",
        "    weighted_score = log_score * 0.5  # Adjust weight as needed\n",
        "\n",
        "    # Calculate initial marks based on the weighted sum\n",
        "    final_marks_obtained = weighted_score * marks  # Scale by full marks\n",
        "\n",
        "    # Apply penalty if marks exceed 90% of full marks\n",
        "    max_allowed_marks = marks * 0.9\n",
        "    if final_marks_obtained > max_allowed_marks:\n",
        "        penalty_factor = 0.8  # Apply a 20% penalty\n",
        "        final_marks_obtained *= penalty_factor\n",
        "\n",
        "    # Ensure marks are never below 0\n",
        "    final_marks_obtained = max(final_marks_obtained, 0)\n",
        "\n",
        "    # Calculate final marks percentage (ensure marks is greater than 0)\n",
        "   # final_marks_percentage = round((final_marks_obtained / marks) * 100, 2) if marks > 0 else 0\n",
        "\n",
        "    # Calculate score percentage (ensure marks is greater than 0)\n",
        "   # score_percentage = round((score / marks) * 100, 2) if marks > 0 else 0\n",
        "\n",
        "    # Analyze student's answer\n",
        "    student_word_count = len(student_answer.split())  # Count words in student's answer\n",
        "    student_sentence_count = len(re.split(r'[.!?]+', student_answer.strip())) - 1  # Count sentences\n",
        "\n",
        "    # Plagiarism and AI-generated text detection\n",
        "    results_plagiarism = plagiarism_checker(student_answer) or {}\n",
        "    result_AI = detect_ai_generated_text_advanced_v3(student_answer, threshold=0.7) or {}\n",
        "\n",
        "    # Prepare final result dictionary\n",
        "    result_data = {\n",
        "       # \"question\": question,\n",
        "       # \"student_answer\": student_answer,\n",
        "       # \"expected_answer\": expected_answer,\n",
        "        \"full_marks\": marks,\n",
        "        \"obtained_marks\": score_old,\n",
        "        \"strengths\": strengths,\n",
        "        \"areas_for_improvement\": areas_for_improvement,\n",
        "        \"clarity_score\": clarity_score,\n",
        "        \"depth_score\": depth_score,\n",
        "\n",
        "       # \"final_obtained_marks\": score,\n",
        "       # \"score_percentage\": score_percentage,\n",
        "        \"final_marks_obtained\": round_based_on_fraction(final_marks_obtained),\n",
        "        \"feedback\": feedback,\n",
        "       # \"final_marks_percentage\": final_marks_percentage,\n",
        "        \"student_word_count\": student_word_count,\n",
        "        \"student_sentence_count\": student_sentence_count,\n",
        "\n",
        "        # Plagiarism-related fields, returned individually\n",
        "        \"plagiarism\": results_plagiarism.get(\"plagiarism\", False),\n",
        "        \"average_score\": results_plagiarism.get(\"average_score\", 0.0),\n",
        "        \"max_score\": results_plagiarism.get(\"max_score\", 0.0),\n",
        "        \"max_score_url\": results_plagiarism.get(\"max_score_url\", \"\"),\n",
        "       # \"plagiarism_data\": results_plagiarism.get(\"data\", \"\"),\n",
        "\n",
        "        # AI detection-related fields\n",
        "        \"AI_category\": result_AI.get(\"category\", \"Unknown\"),\n",
        "        \"AI_flag\": result_AI.get(\"AI\", False),\n",
        "        \"AI_percentage\": result_AI.get(\"AI_Percentage\", 0.0),\n",
        "       # \"AI_confidence\": result_AI.get(\"confidence\", 0.0),\n",
        "       # \"AI_confidence_difference\": result_AI.get(\"confidence_difference\", 0.0),\n",
        "        \"AI_probabilities\": result_AI.get(\"probabilities\", {}),\n",
        "       # \"AI_metrics\": result_AI.get(\"metrics\", {}),\n",
        "       # \"AI_threshold_used\": result_AI.get(\"threshold_used\", 0.0),\n",
        "    }\n",
        "\n",
        "    return result_data"
      ],
      "metadata": {
        "id": "2Y2e9HMwKMIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def extract_score_and_feedback(generated_text):\n",
        "    \"\"\"\n",
        "    Extract detailed evaluation information from the generated text using regular expressions.\n",
        "\n",
        "    :param generated_text: The raw output from the model.\n",
        "    :return: A dictionary with extracted evaluation details.\n",
        "    \"\"\"\n",
        "    # Define the patterns for each field\n",
        "    score_pattern = r'\"marks_obtained\":\\s*([0-9.]+)'  # Extract numerical score\n",
        "    feedback_pattern = r'\"feedback\":\\s*\"([^\"]+)\"'  # Extract feedback text, handling newlines\n",
        "    strengths_pattern = r'\"strengths\":\\s*\\[([^\\]]+)\\]'  # Extract strengths list\n",
        "    areas_for_improvement_pattern = r'\"areas_for_improvement\":\\s*\\[([^\\]]+)\\]'  # Extract areas for improvement list\n",
        "    clarity_score_pattern = r'\"clarity_score\":\\s*([0-9.]+)'  # Extract clarity score\n",
        "    depth_score_pattern = r'\"depth_score\":\\s*([0-9.]+)'  # Extract depth score\n",
        "\n",
        "    # Find matches using regular expressions\n",
        "    score_match = re.search(score_pattern, generated_text)\n",
        "    feedback_match = re.search(feedback_pattern, generated_text)\n",
        "    strengths_match = re.search(strengths_pattern, generated_text)\n",
        "    areas_for_improvement_match = re.search(areas_for_improvement_pattern, generated_text)\n",
        "    clarity_score_match = re.search(clarity_score_pattern, generated_text)\n",
        "    depth_score_match = re.search(depth_score_pattern, generated_text)\n",
        "\n",
        "    # Extract values or set them to None if not found\n",
        "    score = float(score_match.group(1)) if score_match else None\n",
        "    feedback = feedback_match.group(1).strip() if feedback_match else None  # Strip to remove extra spaces/newlines\n",
        "\n",
        "    # Handle strengths and areas for improvement (could be a list or a single string)\n",
        "    strengths = parse_list_or_string(strengths_match.group(1)) if strengths_match else []\n",
        "    areas_for_improvement = parse_list_or_string(areas_for_improvement_match.group(1)) if areas_for_improvement_match else []\n",
        "\n",
        "    clarity_score = float(clarity_score_match.group(1)) if clarity_score_match else None\n",
        "    depth_score = float(depth_score_match.group(1)) if depth_score_match else None\n",
        "\n",
        "    # Return the extracted data as a dictionary\n",
        "    return {\n",
        "        \"marks_obtained\": score,\n",
        "        \"feedback\": feedback,\n",
        "        \"strengths\": strengths,\n",
        "        \"areas_for_improvement\": areas_for_improvement,\n",
        "        \"clarity_score\": clarity_score,\n",
        "        \"depth_score\": depth_score\n",
        "    }\n",
        "\n",
        "def parse_list_or_string(value):\n",
        "    \"\"\"\n",
        "    Parse a field that could either be a list of strings or a single string.\n",
        "    If it's a list, it will return a list; if it's a single string, it will return a list with one item.\n",
        "\n",
        "    :param value: The raw string to be parsed (could be a list or a string).\n",
        "    :return: A list of strings.\n",
        "    \"\"\"\n",
        "    # If the value looks like a list (contains commas), split it into a list\n",
        "    if isinstance(value, str) and \",\" in value:\n",
        "        # Remove leading/trailing whitespace and split by commas\n",
        "        return [item.strip().strip('\"') for item in value.split(\",\")]\n",
        "    # Otherwise, return a list containing the value as a single item\n",
        "    elif isinstance(value, str):\n",
        "        return [value.strip().strip('\"')]\n",
        "    return []"
      ],
      "metadata": {
        "id": "PHNDakykLcPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage for testing\n",
        "question = \"What is the role of the operating system in memory management?\"\n",
        "answer = \"\"\"\n",
        "\n",
        "An Operating System is a System software that manages all the resources of the computing deice.\n",
        "\n",
        "Acts as an interface between the software and different parts of the computer or the computer hardware.\n",
        "Manages the overall resources and operations of the computer.\n",
        "Controls and monitors the execution of all other programs that reside in the computer, which also includes application programs and other system software of the computer.\n",
        "Examples of Operating Systems are Windows, Linux, macOS, Android, iOS, etc.\n",
        "\n",
        "\"\"\"\n",
        "full_marks = 5\n",
        "expected_answer = \" \"  # Assuming expected_answer is a string (could be blank for testing purposes)\n",
        "\n",
        "# Get the final result\n",
        "final_result = GetData(question, answer, expected_answer, full_marks)\n",
        "\n",
        "\n",
        "# Print the final JSON result vertically\n",
        "print(\"Final JSON Result:\")\n",
        "for key, value in final_result.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyoQsAvjKYg1",
        "outputId": "ccd97cba-86c7-4eff-b55f-cdda96a03913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final JSON Result:\n",
            "full_marks: 5\n",
            "obtained_marks: 4.0\n",
            "strengths: ['Correctness', 'Clarity']\n",
            "areas_for_improvement: ['Completeness', 'Depth']\n",
            "clarity_score: 8.0\n",
            "depth_score: 6.0\n",
            "final_marks_obtained: 4\n",
            "feedback: You have answered all the parts correctly, but you need to provide more detail in your answer.\n",
            "student_word_count: 79\n",
            "student_sentence_count: 5\n",
            "plagiarism: False\n",
            "average_score: 0.0\n",
            "max_score: 0\n",
            "max_score_url: \n",
            "AI_category: AI-generated\n",
            "AI_flag: True\n",
            "AI_percentage: 99.64\n",
            "AI_probabilities: {'Human-written': 0.0036265342, 'AI-generated': 0.9963735}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generated_text = evaluate_answer(question, answer, full_marks, expected_answer)\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJhqD3l2QjGo",
        "outputId": "0c5bb0ce-0125-4685-d7e1-2c49a28d8768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Question: What is the role of the operating system in memory management?\n",
            "    Answer: \n",
            "\n",
            "An Operating System is a System software that manages all the resources of the computing deice.\n",
            "\n",
            "Acts as an interface between the software and different parts of the computer or the computer hardware.\n",
            "Manages the overall resources and operations of the computer. \n",
            "Controls and monitors the execution of all other programs that reside in the computer, which also includes application programs and other system software of the computer.\n",
            "Examples of Operating Systems are Windows, Linux, macOS, Android, iOS, etc.\n",
            "\n",
            "\n",
            "    Full Marks: 5\n",
            "    Expected Answer:  \n",
            "\n",
            "    Evaluate based on strictly: Correctness of answer, Completeness of answer, Clarity, Depth, and Relevance of answer , if answer not according to asked question or giving nearly similar answer just give it a 0 marks.\n",
            "    \n",
            "    Return JSON with:\n",
            "    - marks_obtained\n",
            "    - feedback  ( detailed overall feedback )\n",
            "    - strengths ( detailed positive aspects )\n",
            "    - areas_for_improvemen ( for getting full marks)\n",
            "    - clarity_score (1-10)\n",
            "    - depth_score (1-10)\n",
            "    \n",
            "    Answers:\n",
            "    {\n",
            "        \"marks_obtained\": 5,\n",
            "        \"feedback\": \"Great job! Your answer is correct and comprehensive. It covers all the important points. It is easy to understand and clearly written. Your answer is also relevant to the question. Well done!\",\n",
            "        \"strengths\": [\n",
            "            \"Correct answer\",\n",
            "            \"Comprehensive answer\",\n",
            "            \"Easy to understand\",\n",
            "            \"Relevant to the question\",\n",
            "            \"Good use of language\"\n",
            "        ],\n",
            "        \"areas_for_improvement\": [\n",
            "            \"Nothing\"\n",
            "        ],\n",
            "        \"clarity_score\": 9,\n",
            "        \"depth_score\": 9\n",
            "    }\n"
          ]
        }
      ]
    }
  ]
}