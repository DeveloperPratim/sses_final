{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmmGHKgGusgKIfyzlYVHsQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeveloperPratim/sses_final/blob/main/Subjective_Student_Evolution_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Hugging Face Transformers\n",
        "!pip install transformers\n",
        "\n",
        "# For Sentence Transformers\n",
        "!pip install sentence-transformers\n",
        "\n",
        "# For BERTScore\n",
        "!pip install bert-score\n",
        "\n",
        "# For Scipy (includes euclidean and cityblock)\n",
        "!pip install scipy\n",
        "\n",
        "# For Scikit-learn (includes cosine_similarity)\n",
        "!pip install scikit-learn\n",
        "\n",
        "# For NumPy (used for standard deviation)\n",
        "!pip install numpy"
      ],
      "metadata": {
        "id": "4quzAr92hmuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MyModel For Marks Calculation"
      ],
      "metadata": {
        "id": "ZMtflwpQhZa-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nmLU19_hSdj"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Define your Google Drive path\n",
        "drive_path = \"/content/drive/MyDrive/MyModel\"\n",
        "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=drive_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=drive_path)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model For Simentic Similarity Calculation"
      ],
      "metadata": {
        "id": "2V63Wh6chejU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "# Initialize models if not already initialized\n",
        "try:\n",
        "    # Check for SentenceTransformer model\n",
        "    if 'sentence_transformer_model' not in globals():\n",
        "        sentence_transformer_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        print(\"SentenceTransformer model initialized.\")\n",
        "    else:\n",
        "        print(\"SentenceTransformer model already initialized.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during model initialization: {e}\")"
      ],
      "metadata": {
        "id": "UBD5gxP9hiM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining All Functions Required"
      ],
      "metadata": {
        "id": "A9QmndwihsLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import euclidean, cityblock\n",
        "from scipy.stats import pearsonr\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import bert_score\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "\n",
        "def extract_score_feedback_and_similarity(generated_text, semantic_similarity):\n",
        "    # Regular expression patterns\n",
        "    score_pattern = r'\"score\":\\s*([0-9.]+)'\n",
        "    feedback_pattern = r'\"feedback\":\\s*\"([^\"]+)\"'\n",
        "\n",
        "    # Find matches\n",
        "    score_match = re.search(score_pattern, generated_text)\n",
        "    feedback_match = re.search(feedback_pattern, generated_text)\n",
        "\n",
        "    # Extract values\n",
        "    score = float(score_match.group(1)) if score_match else None\n",
        "    feedback = feedback_match.group(1) if feedback_match else None\n",
        "\n",
        "    # Return the result with semantic_similarity\n",
        "    return {\n",
        "        \"score\": score,\n",
        "        \"feedback\": feedback,\n",
        "        \"semantic_similarity\": semantic_similarity\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_similarity(correct_answer, student_answer, model_name=\"all-MiniLM-L6-v2\"):\n",
        "    # Encode the answers into embeddings (vector representations)\n",
        "    embeddings = sentence_transformer_model.encode([correct_answer, student_answer])\n",
        "\n",
        "    # Cosine similarity\n",
        "    cosine_sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "\n",
        "    # Euclidean distance\n",
        "    euclidean_dist = euclidean(embeddings[0], embeddings[1])\n",
        "\n",
        "    # Manhattan distance\n",
        "    manhattan_dist = cityblock(embeddings[0], embeddings[1])\n",
        "\n",
        "    # Pearson correlation\n",
        "    pearson_corr, _ = pearsonr(embeddings[0], embeddings[1])\n",
        "\n",
        "    # Jaccard Similarity\n",
        "    set_correct = set(correct_answer.split())\n",
        "    set_student = set(student_answer.split())\n",
        "    jaccard_sim = len(set_correct.intersection(set_student)) / len(set_correct.union(set_student))\n",
        "\n",
        "    # BERTScore\n",
        "    P, R, F1 = bert_score.score([correct_answer], [student_answer], lang=\"en\")\n",
        "    bert_score_sim = F1.mean().item()\n",
        "\n",
        "    return {\n",
        "        \"cosine_similarity\": cosine_sim,\n",
        "        \"euclidean_distance\": euclidean_dist,\n",
        "        \"manhattan_distance\": manhattan_dist,\n",
        "        \"pearson_correlation\": pearson_corr,\n",
        "        \"jaccard_similarity\": jaccard_sim,\n",
        "        \"bert_score_similarity\": bert_score_sim\n",
        "    }\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def compute_final_marks(similarities, max_marks, score, feedback, question, student_answer, actual_answer):\n",
        "    # Apply adjustments to score based on the provided conditions\n",
        "    if score < 1.5:\n",
        "        score -= 0.92\n",
        "    elif score < 3.5:\n",
        "        score -= 0.56\n",
        "    elif score < 4.8:\n",
        "        score -= 0.12\n",
        "\n",
        "    # Normalize the score after adjustments\n",
        "    adjusted_score = max(0, min(score, max_marks))  # Ensure score is within 0 to max_marks\n",
        "\n",
        "    # Standard deviation of the similarity scores (using the list of similarity scores)\n",
        "    similarity_values = [\n",
        "        similarities['cosine_similarity'],\n",
        "        similarities['pearson_correlation'],\n",
        "        similarities['bert_score_similarity'],\n",
        "        similarities['jaccard_similarity'],\n",
        "        similarities['euclidean_distance'],\n",
        "        similarities['manhattan_distance']\n",
        "    ]\n",
        "\n",
        "    # Calculate the standard deviation (SD) of the similarity scores\n",
        "    similarity_sd = np.std(similarity_values)\n",
        "\n",
        "    # Scale or limit similarity_sd adjustment\n",
        "    similarity_sd = min(similarity_sd, 0.5 * max_marks)  # Limit the impact of SD\n",
        "\n",
        "    # Adjust the final score based on similarity SD\n",
        "    final_score = adjusted_score - similarity_sd\n",
        "\n",
        "    # Ensure the final score does not exceed max_marks\n",
        "    final_marks = max(0, min(final_score, max_marks))\n",
        "\n",
        "    # Calculate percentage (scaled to max_marks)\n",
        "    percentage = (final_marks / max_marks) * 100\n",
        "\n",
        "    # Return the results with necessary fields\n",
        "    return {\n",
        "        'final_marks': final_marks,\n",
        "        'max_marks': max_marks,\n",
        "        'marks_obtained': adjusted_score,\n",
        "        'feedback': feedback,\n",
        "        'question': question,\n",
        "        'student_answer': student_answer,\n",
        "        'actual_answer': actual_answer,\n",
        "        'similarity_sd': similarity_sd,\n",
        "        'percentage': percentage\n",
        "    }\n",
        "\n",
        "\n",
        "def get_data(question, student_answer, actual_answer, max_marks):\n",
        "    return {\n",
        "        'question': question,\n",
        "        'student_answer': student_answer,\n",
        "        'actual_answer': actual_answer,\n",
        "        'max_marks': max_marks\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_student_answer(pipe, question, correct_answer, student_answer, max_marks):\n",
        "    # Dynamically set max token limit\n",
        "    input_token_count = len(pipe.tokenizer.encode(f\"Question: {question} Answer: {student_answer}\"))\n",
        "    max_allowed_tokens = 1024  # Adjust based on your model's total token limit\n",
        "    max_new_tokens = max(50, max_allowed_tokens - input_token_count)  # Ensure a minimum response length\n",
        "\n",
        "    input_text = f\"\"\"\n",
        "    Question: {question}\n",
        "    Answer: {student_answer}\n",
        "    Evaluate and provide these two fields **score** and **feedback** based on this answer where full marks is 5 and can be given for most precious and best answer and 0 for wrong or irrelevant answer and in response just give two fields in json, with no extra text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate output with dynamic token limit from model\n",
        "    result = pipe(input_text, max_new_tokens=max_new_tokens, num_return_sequences=1, truncation=True)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract score and feedback from model output\n",
        "    semantic_similarity = compute_similarity(correct_answer, student_answer)[\"cosine_similarity\"]\n",
        "    score_feedback = extract_score_feedback_and_similarity(generated_text, semantic_similarity)\n",
        "\n",
        "    global score\n",
        "    # Use the score and feedback from model\n",
        "    score = score_feedback[\"score\"]\n",
        "    feedback = score_feedback[\"feedback\"]\n",
        "\n",
        "    # Collect similarities for final marks computation\n",
        "    similarities = compute_similarity(correct_answer, student_answer)\n",
        "\n",
        "    # Get the required data for final marks calculation\n",
        "    data = get_data(question, student_answer, correct_answer, max_marks)\n",
        "\n",
        "    # Compute final marks and return all values\n",
        "    final_result = compute_final_marks(similarities, max_marks, score, feedback, data['question'], data['student_answer'], data['actual_answer'])\n",
        "\n",
        "    # Print the final result in a more readable format\n",
        "    print(f\"Question: {final_result['question']}\")\n",
        "    print(f\"Student's Answer: {final_result['student_answer']}\")\n",
        "    print(f\"Correct Answer: {final_result['actual_answer']}\")\n",
        "    print(f\"Maximum Marks: {final_result['max_marks']}\")\n",
        "    print(f\"Marks Obtained: {final_result['marks_obtained']}\")\n",
        "    print(f\"Final Marks: {final_result['final_marks']}\")\n",
        "    print(f\"Feedback: {final_result['feedback']}\")\n",
        "    print(f\"Percentage: {final_result['marks_obtained'] / final_result['max_marks'] * 100:.2f}%\")\n",
        "    print(f\"Similarity Metrics:\")\n",
        "    print(f\"   Cosine Similarity: {similarities['cosine_similarity']:.2f}\")\n",
        "    print(f\"   Pearson Correlation: {similarities['pearson_correlation']:.2f}\")\n",
        "    print(f\"   BERTScore Similarity: {similarities['bert_score_similarity']:.2f}\")\n",
        "    print(f\"   Jaccard Similarity: {similarities['jaccard_similarity']:.2f}\")\n",
        "    print(f\"   Euclidean Distance: {similarities['euclidean_distance']:.2f}\")\n",
        "    print(f\"   Manhattan Distance: {similarities['manhattan_distance']:.2f}\")\n",
        "\n",
        "    return final_result"
      ],
      "metadata": {
        "id": "_d8yajvghu9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Those Function to See the Output"
      ],
      "metadata": {
        "id": "ynCD3ZdGh-Ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage\n",
        "question = \"What is the role of an operating system?\"\n",
        "correct_answer = \"The operating system manages the hardware and software resources of a computer.\"\n",
        "student_answer = \"\"\"\n",
        "\n",
        "operating system (OS) manages a computer's hardware and software, and acts as a bridge between the user and the computer. Some of its key roles include:\n",
        "Memory management\n",
        "Allocates and tracks memory for applications and processes, ensuring they have enough to run and that the system performs well. This is especially important for computers with limited memory.\n",
        "Process management\n",
        "Controls how programs interact with each other, and ensures that resources are used efficiently and the system is stable.\n",
        "File system management\n",
        "Organizes and manages files on storage devices, allowing users to create, read, write, and delete files.\n",
        "User interface\n",
        "Provides a user-friendly interface for interacting with the computer, either through a graphical or command-line interface.\n",
        "Other functions of an operating system include: Processor management and scheduling, Device management, and Storage management.\n",
        "\n",
        "\n",
        "\"\"\"  # The student's answer\n",
        "max_marks = 5  # The maximum marks for the question\n",
        "\n",
        "# Assume `pipe` is your language model pipeline\n",
        "final_result = evaluate_student_answer(pipe, question, correct_answer, student_answer, max_marks)"
      ],
      "metadata": {
        "id": "lUGgtODgh9S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Score: {score}\")"
      ],
      "metadata": {
        "id": "YxDlYRJyiPcp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}